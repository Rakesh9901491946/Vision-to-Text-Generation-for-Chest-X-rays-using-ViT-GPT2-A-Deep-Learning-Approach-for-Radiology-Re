{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb2aa773-97aa-48c6-985b-95eb4965c672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/raddar/chest-xrays-indiana-university?dataset_version_number=2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.2G/13.2G [05:49<00:00, 40.5MB/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/jovyan/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2\n"
     ]
    }
   ],
   "source": [
    "# dataset_download.py\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "def download_indiana_dataset():\n",
    "    # Download latest version of the dataset\n",
    "    path = kagglehub.dataset_download(\"raddar/chest-xrays-indiana-university\")\n",
    "    print(\"Path to dataset files:\", path)\n",
    "    return path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_indiana_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "172c4d9d-1539-4198-b68b-0eda3d647e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting kagglehub\n",
      "  Using cached kagglehub-0.3.10-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2024.2.2)\n",
      "Downloading kagglehub-0.3.10-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe54419-c13b-463e-8b85-9527c0d4f320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "998a0ab6686d4343b8cc79e7184c5d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce03782ab0f142138d08d36b00ad28d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c9740fbd524f10a1455d6dfd0d10e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68eb667624646db85d50147f50db99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f875f23e13414587c1df831f438b0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2e438886714143888f859f21af16aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97df745d56544f628f24c4a92715afbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9cf059a9d164e8b8df4f95f60ab3ce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bab5179ee44f84b7c69e648d213196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bc1bca0a8e8414d9119ad63aa298b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VisionEncoderDecoderModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 Step 0/1867 — Loss: 11.8036\n",
      "Epoch 1/20 Step 25/1867 — Loss: 0.4650\n",
      "Epoch 1/20 Step 50/1867 — Loss: 0.2692\n",
      "Epoch 1/20 Step 75/1867 — Loss: 0.5799\n",
      "Epoch 1/20 Step 100/1867 — Loss: 0.0609\n",
      "Epoch 1/20 Step 125/1867 — Loss: 0.4100\n",
      "Epoch 1/20 Step 150/1867 — Loss: 0.1605\n",
      "Epoch 1/20 Step 175/1867 — Loss: 0.3377\n",
      "Epoch 1/20 Step 200/1867 — Loss: 0.7924\n",
      "Epoch 1/20 Step 225/1867 — Loss: 0.0990\n",
      "Epoch 1/20 Step 250/1867 — Loss: 0.0623\n",
      "Epoch 1/20 Step 275/1867 — Loss: 0.3516\n",
      "Epoch 1/20 Step 300/1867 — Loss: 0.6580\n",
      "Epoch 1/20 Step 325/1867 — Loss: 0.4435\n",
      "Epoch 1/20 Step 350/1867 — Loss: 0.3262\n",
      "Epoch 1/20 Step 375/1867 — Loss: 0.6305\n",
      "Epoch 1/20 Step 400/1867 — Loss: 0.3429\n",
      "Epoch 1/20 Step 425/1867 — Loss: 0.3513\n",
      "Epoch 1/20 Step 450/1867 — Loss: 0.1127\n",
      "Epoch 1/20 Step 475/1867 — Loss: 0.0616\n",
      "Epoch 1/20 Step 500/1867 — Loss: 0.0368\n",
      "Epoch 1/20 Step 525/1867 — Loss: 0.0440\n",
      "Epoch 1/20 Step 550/1867 — Loss: 0.1562\n",
      "Epoch 1/20 Step 575/1867 — Loss: 0.2535\n",
      "Epoch 1/20 Step 600/1867 — Loss: 0.5807\n",
      "Epoch 1/20 Step 625/1867 — Loss: 0.1213\n",
      "Epoch 1/20 Step 650/1867 — Loss: 0.7022\n",
      "Epoch 1/20 Step 675/1867 — Loss: 0.0878\n",
      "Epoch 1/20 Step 700/1867 — Loss: 0.3886\n",
      "Epoch 1/20 Step 725/1867 — Loss: 0.3042\n",
      "Epoch 1/20 Step 750/1867 — Loss: 0.1549\n",
      "Epoch 1/20 Step 775/1867 — Loss: 0.0596\n",
      "Epoch 1/20 Step 800/1867 — Loss: 0.5996\n",
      "Epoch 1/20 Step 825/1867 — Loss: 0.0692\n",
      "Epoch 1/20 Step 850/1867 — Loss: 0.0590\n",
      "Epoch 1/20 Step 875/1867 — Loss: 0.0680\n",
      "Epoch 1/20 Step 900/1867 — Loss: 0.0467\n",
      "Epoch 1/20 Step 925/1867 — Loss: 0.1318\n",
      "Epoch 1/20 Step 950/1867 — Loss: 0.5534\n",
      "Epoch 1/20 Step 975/1867 — Loss: 0.1201\n",
      "Epoch 1/20 Step 1000/1867 — Loss: 0.0517\n",
      "Epoch 1/20 Step 1025/1867 — Loss: 0.0834\n",
      "Epoch 1/20 Step 1050/1867 — Loss: 0.2379\n",
      "Epoch 1/20 Step 1075/1867 — Loss: 0.3445\n",
      "Epoch 1/20 Step 1100/1867 — Loss: 0.2266\n",
      "Epoch 1/20 Step 1125/1867 — Loss: 0.2698\n",
      "Epoch 1/20 Step 1150/1867 — Loss: 0.0658\n",
      "Epoch 1/20 Step 1175/1867 — Loss: 0.4818\n",
      "Epoch 1/20 Step 1200/1867 — Loss: 0.1809\n",
      "Epoch 1/20 Step 1225/1867 — Loss: 0.3790\n",
      "Epoch 1/20 Step 1250/1867 — Loss: 0.0879\n",
      "Epoch 1/20 Step 1275/1867 — Loss: 0.1161\n",
      "Epoch 1/20 Step 1300/1867 — Loss: 0.2776\n",
      "Epoch 1/20 Step 1325/1867 — Loss: 0.1647\n",
      "Epoch 1/20 Step 1350/1867 — Loss: 0.1934\n",
      "Epoch 1/20 Step 1375/1867 — Loss: 0.0796\n",
      "Epoch 1/20 Step 1400/1867 — Loss: 0.2431\n",
      "Epoch 1/20 Step 1425/1867 — Loss: 0.0765\n",
      "Epoch 1/20 Step 1450/1867 — Loss: 0.1343\n",
      "Epoch 1/20 Step 1475/1867 — Loss: 0.3178\n",
      "Epoch 1/20 Step 1500/1867 — Loss: 0.4257\n",
      "Epoch 1/20 Step 1525/1867 — Loss: 0.3464\n",
      "Epoch 1/20 Step 1550/1867 — Loss: 0.0680\n",
      "Epoch 1/20 Step 1575/1867 — Loss: 0.5076\n",
      "Epoch 1/20 Step 1600/1867 — Loss: 0.1038\n",
      "Epoch 1/20 Step 1625/1867 — Loss: 0.3494\n",
      "Epoch 1/20 Step 1650/1867 — Loss: 0.4870\n",
      "Epoch 1/20 Step 1675/1867 — Loss: 0.3437\n",
      "Epoch 1/20 Step 1700/1867 — Loss: 0.1899\n",
      "Epoch 1/20 Step 1725/1867 — Loss: 0.0345\n",
      "Epoch 1/20 Step 1750/1867 — Loss: 0.6240\n",
      "Epoch 1/20 Step 1775/1867 — Loss: 0.1767\n",
      "Epoch 1/20 Step 1800/1867 — Loss: 0.2419\n",
      "Epoch 1/20 Step 1825/1867 — Loss: 0.6027\n",
      "Epoch 1/20 Step 1850/1867 — Loss: 0.4944\n",
      "Epoch 2/20 Step 0/1867 — Loss: 0.1886\n",
      "Epoch 2/20 Step 25/1867 — Loss: 0.1795\n",
      "Epoch 2/20 Step 50/1867 — Loss: 0.1876\n",
      "Epoch 2/20 Step 75/1867 — Loss: 0.0322\n",
      "Epoch 2/20 Step 100/1867 — Loss: 0.1597\n",
      "Epoch 2/20 Step 125/1867 — Loss: 0.0838\n",
      "Epoch 2/20 Step 150/1867 — Loss: 0.1503\n",
      "Epoch 2/20 Step 175/1867 — Loss: 0.0890\n",
      "Epoch 2/20 Step 200/1867 — Loss: 0.3020\n",
      "Epoch 2/20 Step 225/1867 — Loss: 0.4344\n",
      "Epoch 2/20 Step 250/1867 — Loss: 0.1735\n",
      "Epoch 2/20 Step 275/1867 — Loss: 0.0662\n",
      "Epoch 2/20 Step 300/1867 — Loss: 0.2230\n",
      "Epoch 2/20 Step 325/1867 — Loss: 0.1444\n",
      "Epoch 2/20 Step 350/1867 — Loss: 0.0614\n",
      "Epoch 2/20 Step 375/1867 — Loss: 0.1449\n",
      "Epoch 2/20 Step 400/1867 — Loss: 0.4577\n",
      "Epoch 2/20 Step 425/1867 — Loss: 0.1082\n",
      "Epoch 2/20 Step 450/1867 — Loss: 0.3206\n",
      "Epoch 2/20 Step 475/1867 — Loss: 0.2085\n",
      "Epoch 2/20 Step 500/1867 — Loss: 0.2195\n",
      "Epoch 2/20 Step 525/1867 — Loss: 0.2292\n",
      "Epoch 2/20 Step 550/1867 — Loss: 0.1657\n",
      "Epoch 2/20 Step 575/1867 — Loss: 0.1418\n",
      "Epoch 2/20 Step 600/1867 — Loss: 0.3230\n",
      "Epoch 2/20 Step 625/1867 — Loss: 0.5421\n",
      "Epoch 2/20 Step 650/1867 — Loss: 0.1127\n",
      "Epoch 2/20 Step 675/1867 — Loss: 0.0838\n",
      "Epoch 2/20 Step 700/1867 — Loss: 0.1834\n",
      "Epoch 2/20 Step 725/1867 — Loss: 0.2675\n",
      "Epoch 2/20 Step 750/1867 — Loss: 0.1896\n",
      "Epoch 2/20 Step 775/1867 — Loss: 0.2370\n",
      "Epoch 2/20 Step 800/1867 — Loss: 0.0668\n",
      "Epoch 2/20 Step 825/1867 — Loss: 0.6679\n",
      "Epoch 2/20 Step 850/1867 — Loss: 0.3047\n",
      "Epoch 2/20 Step 875/1867 — Loss: 0.3896\n",
      "Epoch 2/20 Step 900/1867 — Loss: 0.0369\n",
      "Epoch 2/20 Step 925/1867 — Loss: 0.2329\n",
      "Epoch 2/20 Step 950/1867 — Loss: 0.0517\n",
      "Epoch 2/20 Step 975/1867 — Loss: 0.3582\n",
      "Epoch 2/20 Step 1000/1867 — Loss: 0.3413\n",
      "Epoch 2/20 Step 1025/1867 — Loss: 0.0619\n",
      "Epoch 2/20 Step 1050/1867 — Loss: 0.1127\n",
      "Epoch 2/20 Step 1075/1867 — Loss: 0.0297\n",
      "Epoch 2/20 Step 1100/1867 — Loss: 0.5488\n",
      "Epoch 2/20 Step 1125/1867 — Loss: 0.1789\n",
      "Epoch 2/20 Step 1150/1867 — Loss: 0.4594\n",
      "Epoch 2/20 Step 1175/1867 — Loss: 0.3813\n",
      "Epoch 2/20 Step 1200/1867 — Loss: 0.4025\n",
      "Epoch 2/20 Step 1225/1867 — Loss: 0.0428\n",
      "Epoch 2/20 Step 1250/1867 — Loss: 0.4353\n",
      "Epoch 2/20 Step 1275/1867 — Loss: 0.0838\n",
      "Epoch 2/20 Step 1300/1867 — Loss: 0.1660\n",
      "Epoch 2/20 Step 1325/1867 — Loss: 0.0827\n",
      "Epoch 2/20 Step 1350/1867 — Loss: 0.1462\n",
      "Epoch 2/20 Step 1375/1867 — Loss: 0.1535\n",
      "Epoch 2/20 Step 1400/1867 — Loss: 0.4573\n",
      "Epoch 2/20 Step 1425/1867 — Loss: 0.2239\n",
      "Epoch 2/20 Step 1450/1867 — Loss: 0.2025\n",
      "Epoch 2/20 Step 1475/1867 — Loss: 0.0306\n",
      "Epoch 2/20 Step 1500/1867 — Loss: 0.2907\n",
      "Epoch 2/20 Step 1525/1867 — Loss: 0.0578\n",
      "Epoch 2/20 Step 1550/1867 — Loss: 0.0519\n",
      "Epoch 2/20 Step 1575/1867 — Loss: 0.1504\n",
      "Epoch 2/20 Step 1600/1867 — Loss: 0.1479\n",
      "Epoch 2/20 Step 1625/1867 — Loss: 0.3454\n",
      "Epoch 2/20 Step 1650/1867 — Loss: 0.4421\n",
      "Epoch 2/20 Step 1675/1867 — Loss: 0.1773\n",
      "Epoch 2/20 Step 1700/1867 — Loss: 0.0548\n",
      "Epoch 2/20 Step 1725/1867 — Loss: 0.0717\n",
      "Epoch 2/20 Step 1750/1867 — Loss: 0.4753\n",
      "Epoch 2/20 Step 1775/1867 — Loss: 0.0970\n",
      "Epoch 2/20 Step 1800/1867 — Loss: 0.3268\n",
      "Epoch 2/20 Step 1825/1867 — Loss: 0.1540\n",
      "Epoch 2/20 Step 1850/1867 — Loss: 0.3128\n",
      "Epoch 3/20 Step 0/1867 — Loss: 0.0961\n",
      "Epoch 3/20 Step 25/1867 — Loss: 0.0556\n",
      "Epoch 3/20 Step 50/1867 — Loss: 0.0541\n",
      "Epoch 3/20 Step 75/1867 — Loss: 0.0323\n",
      "Epoch 3/20 Step 100/1867 — Loss: 0.2074\n",
      "Epoch 3/20 Step 125/1867 — Loss: 0.2848\n",
      "Epoch 3/20 Step 150/1867 — Loss: 0.1252\n",
      "Epoch 3/20 Step 175/1867 — Loss: 0.4772\n",
      "Epoch 3/20 Step 200/1867 — Loss: 0.6942\n",
      "Epoch 3/20 Step 225/1867 — Loss: 0.1884\n",
      "Epoch 3/20 Step 250/1867 — Loss: 0.1327\n",
      "Epoch 3/20 Step 275/1867 — Loss: 0.3221\n",
      "Epoch 3/20 Step 300/1867 — Loss: 0.0372\n",
      "Epoch 3/20 Step 325/1867 — Loss: 0.0590\n",
      "Epoch 3/20 Step 350/1867 — Loss: 0.1431\n",
      "Epoch 3/20 Step 375/1867 — Loss: 0.0425\n",
      "Epoch 3/20 Step 400/1867 — Loss: 0.0403\n",
      "Epoch 3/20 Step 425/1867 — Loss: 0.0370\n",
      "Epoch 3/20 Step 450/1867 — Loss: 0.5378\n",
      "Epoch 3/20 Step 475/1867 — Loss: 0.0583\n",
      "Epoch 3/20 Step 500/1867 — Loss: 0.1488\n",
      "Epoch 3/20 Step 525/1867 — Loss: 0.3287\n",
      "Epoch 3/20 Step 550/1867 — Loss: 0.2242\n",
      "Epoch 3/20 Step 575/1867 — Loss: 0.2123\n",
      "Epoch 3/20 Step 600/1867 — Loss: 0.0705\n",
      "Epoch 3/20 Step 625/1867 — Loss: 0.1093\n",
      "Epoch 3/20 Step 650/1867 — Loss: 0.0381\n",
      "Epoch 3/20 Step 675/1867 — Loss: 0.1635\n",
      "Epoch 3/20 Step 700/1867 — Loss: 0.1555\n",
      "Epoch 3/20 Step 725/1867 — Loss: 0.0598\n",
      "Epoch 3/20 Step 750/1867 — Loss: 0.2201\n",
      "Epoch 3/20 Step 775/1867 — Loss: 0.1353\n",
      "Epoch 3/20 Step 800/1867 — Loss: 0.0867\n",
      "Epoch 3/20 Step 825/1867 — Loss: 0.0858\n",
      "Epoch 3/20 Step 850/1867 — Loss: 0.1637\n",
      "Epoch 3/20 Step 875/1867 — Loss: 0.1825\n",
      "Epoch 3/20 Step 900/1867 — Loss: 0.2424\n",
      "Epoch 3/20 Step 925/1867 — Loss: 0.4141\n",
      "Epoch 3/20 Step 950/1867 — Loss: 0.2515\n",
      "Epoch 3/20 Step 975/1867 — Loss: 0.0726\n",
      "Epoch 3/20 Step 1000/1867 — Loss: 0.3193\n",
      "Epoch 3/20 Step 1025/1867 — Loss: 0.0930\n",
      "Epoch 3/20 Step 1050/1867 — Loss: 0.0435\n",
      "Epoch 3/20 Step 1075/1867 — Loss: 0.1700\n",
      "Epoch 3/20 Step 1100/1867 — Loss: 0.0778\n",
      "Epoch 3/20 Step 1125/1867 — Loss: 0.1333\n",
      "Epoch 3/20 Step 1150/1867 — Loss: 0.1540\n",
      "Epoch 3/20 Step 1175/1867 — Loss: 0.1099\n",
      "Epoch 3/20 Step 1200/1867 — Loss: 0.1421\n",
      "Epoch 3/20 Step 1225/1867 — Loss: 0.0706\n",
      "Epoch 3/20 Step 1250/1867 — Loss: 0.1700\n",
      "Epoch 3/20 Step 1275/1867 — Loss: 0.3127\n",
      "Epoch 3/20 Step 1300/1867 — Loss: 0.1385\n",
      "Epoch 3/20 Step 1325/1867 — Loss: 0.1044\n",
      "Epoch 3/20 Step 1350/1867 — Loss: 0.2082\n",
      "Epoch 3/20 Step 1375/1867 — Loss: 0.1135\n",
      "Epoch 3/20 Step 1400/1867 — Loss: 0.1486\n",
      "Epoch 3/20 Step 1425/1867 — Loss: 0.1516\n",
      "Epoch 3/20 Step 1450/1867 — Loss: 0.1527\n",
      "Epoch 3/20 Step 1475/1867 — Loss: 0.1496\n",
      "Epoch 3/20 Step 1500/1867 — Loss: 0.1217\n",
      "Epoch 3/20 Step 1525/1867 — Loss: 0.1392\n",
      "Epoch 3/20 Step 1550/1867 — Loss: 0.5522\n",
      "Epoch 3/20 Step 1575/1867 — Loss: 0.1492\n",
      "Epoch 3/20 Step 1600/1867 — Loss: 0.1514\n",
      "Epoch 3/20 Step 1625/1867 — Loss: 0.1004\n",
      "Epoch 3/20 Step 1650/1867 — Loss: 0.0573\n",
      "Epoch 3/20 Step 1675/1867 — Loss: 0.1278\n",
      "Epoch 3/20 Step 1700/1867 — Loss: 0.2263\n",
      "Epoch 3/20 Step 1725/1867 — Loss: 0.1049\n",
      "Epoch 3/20 Step 1750/1867 — Loss: 0.0617\n",
      "Epoch 3/20 Step 1775/1867 — Loss: 0.1442\n",
      "Epoch 3/20 Step 1800/1867 — Loss: 0.1498\n",
      "Epoch 3/20 Step 1825/1867 — Loss: 0.2474\n",
      "Epoch 3/20 Step 1850/1867 — Loss: 0.0475\n",
      "Epoch 4/20 Step 0/1867 — Loss: 0.2734\n",
      "Epoch 4/20 Step 25/1867 — Loss: 0.0380\n",
      "Epoch 4/20 Step 50/1867 — Loss: 0.1753\n",
      "Epoch 4/20 Step 75/1867 — Loss: 0.0677\n",
      "Epoch 4/20 Step 100/1867 — Loss: 0.0663\n",
      "Epoch 4/20 Step 125/1867 — Loss: 0.1375\n",
      "Epoch 4/20 Step 150/1867 — Loss: 0.0961\n",
      "Epoch 4/20 Step 175/1867 — Loss: 0.1270\n",
      "Epoch 4/20 Step 200/1867 — Loss: 0.0622\n",
      "Epoch 4/20 Step 225/1867 — Loss: 0.1067\n",
      "Epoch 4/20 Step 250/1867 — Loss: 0.1883\n",
      "Epoch 4/20 Step 275/1867 — Loss: 0.1399\n",
      "Epoch 4/20 Step 300/1867 — Loss: 0.2665\n",
      "Epoch 4/20 Step 325/1867 — Loss: 0.1300\n",
      "Epoch 4/20 Step 350/1867 — Loss: 0.0589\n",
      "Epoch 4/20 Step 375/1867 — Loss: 0.0945\n",
      "Epoch 4/20 Step 400/1867 — Loss: 0.2136\n",
      "Epoch 4/20 Step 425/1867 — Loss: 0.2325\n",
      "Epoch 4/20 Step 450/1867 — Loss: 0.1143\n",
      "Epoch 4/20 Step 475/1867 — Loss: 0.0402\n",
      "Epoch 4/20 Step 500/1867 — Loss: 0.0844\n",
      "Epoch 4/20 Step 525/1867 — Loss: 0.0386\n",
      "Epoch 4/20 Step 550/1867 — Loss: 0.0297\n",
      "Epoch 4/20 Step 575/1867 — Loss: 0.0756\n",
      "Epoch 4/20 Step 600/1867 — Loss: 0.0537\n",
      "Epoch 4/20 Step 625/1867 — Loss: 0.0461\n",
      "Epoch 4/20 Step 650/1867 — Loss: 0.2571\n",
      "Epoch 4/20 Step 675/1867 — Loss: 0.0630\n",
      "Epoch 4/20 Step 700/1867 — Loss: 0.0806\n",
      "Epoch 4/20 Step 725/1867 — Loss: 0.0626\n",
      "Epoch 4/20 Step 750/1867 — Loss: 0.1309\n",
      "Epoch 4/20 Step 775/1867 — Loss: 0.2188\n",
      "Epoch 4/20 Step 800/1867 — Loss: 0.0848\n",
      "Epoch 4/20 Step 825/1867 — Loss: 0.0888\n",
      "Epoch 4/20 Step 850/1867 — Loss: 0.0907\n",
      "Epoch 4/20 Step 875/1867 — Loss: 0.2322\n",
      "Epoch 4/20 Step 900/1867 — Loss: 0.0963\n",
      "Epoch 4/20 Step 925/1867 — Loss: 0.1604\n",
      "Epoch 4/20 Step 950/1867 — Loss: 0.1007\n",
      "Epoch 4/20 Step 975/1867 — Loss: 0.1714\n",
      "Epoch 4/20 Step 1000/1867 — Loss: 0.0897\n",
      "Epoch 4/20 Step 1025/1867 — Loss: 0.1448\n",
      "Epoch 4/20 Step 1050/1867 — Loss: 0.1409\n",
      "Epoch 4/20 Step 1075/1867 — Loss: 0.1558\n",
      "Epoch 4/20 Step 1100/1867 — Loss: 0.1911\n",
      "Epoch 4/20 Step 1125/1867 — Loss: 0.1384\n",
      "Epoch 4/20 Step 1150/1867 — Loss: 0.0546\n",
      "Epoch 4/20 Step 1175/1867 — Loss: 0.0976\n",
      "Epoch 4/20 Step 1200/1867 — Loss: 0.1069\n",
      "Epoch 4/20 Step 1225/1867 — Loss: 0.1161\n",
      "Epoch 4/20 Step 1250/1867 — Loss: 0.0557\n",
      "Epoch 4/20 Step 1275/1867 — Loss: 0.0841\n",
      "Epoch 4/20 Step 1300/1867 — Loss: 0.0883\n",
      "Epoch 4/20 Step 1325/1867 — Loss: 0.1710\n",
      "Epoch 4/20 Step 1350/1867 — Loss: 0.0903\n",
      "Epoch 4/20 Step 1375/1867 — Loss: 0.0392\n",
      "Epoch 4/20 Step 1400/1867 — Loss: 0.1790\n",
      "Epoch 4/20 Step 1425/1867 — Loss: 0.0469\n",
      "Epoch 4/20 Step 1450/1867 — Loss: 0.1410\n",
      "Epoch 4/20 Step 1475/1867 — Loss: 0.0970\n",
      "Epoch 4/20 Step 1500/1867 — Loss: 0.0520\n",
      "Epoch 4/20 Step 1525/1867 — Loss: 0.0468\n",
      "Epoch 4/20 Step 1550/1867 — Loss: 0.1493\n",
      "Epoch 4/20 Step 1575/1867 — Loss: 0.1727\n",
      "Epoch 4/20 Step 1600/1867 — Loss: 0.0954\n",
      "Epoch 4/20 Step 1625/1867 — Loss: 0.2248\n",
      "Epoch 4/20 Step 1650/1867 — Loss: 0.0994\n",
      "Epoch 4/20 Step 1675/1867 — Loss: 0.1183\n",
      "Epoch 4/20 Step 1700/1867 — Loss: 0.0375\n",
      "Epoch 4/20 Step 1725/1867 — Loss: 0.0808\n",
      "Epoch 4/20 Step 1750/1867 — Loss: 0.0714\n",
      "Epoch 4/20 Step 1775/1867 — Loss: 0.0558\n",
      "Epoch 4/20 Step 1800/1867 — Loss: 0.0822\n",
      "Epoch 4/20 Step 1825/1867 — Loss: 0.3066\n",
      "Epoch 4/20 Step 1850/1867 — Loss: 0.0943\n",
      "Epoch 5/20 Step 0/1867 — Loss: 0.1768\n",
      "Epoch 5/20 Step 25/1867 — Loss: 0.1182\n",
      "Epoch 5/20 Step 50/1867 — Loss: 0.1197\n",
      "Epoch 5/20 Step 75/1867 — Loss: 0.1210\n",
      "Epoch 5/20 Step 100/1867 — Loss: 0.0842\n",
      "Epoch 5/20 Step 125/1867 — Loss: 0.0915\n",
      "Epoch 5/20 Step 150/1867 — Loss: 0.1715\n",
      "Epoch 5/20 Step 175/1867 — Loss: 0.1011\n",
      "Epoch 5/20 Step 200/1867 — Loss: 0.2063\n",
      "Epoch 5/20 Step 225/1867 — Loss: 0.0736\n",
      "Epoch 5/20 Step 250/1867 — Loss: 0.1374\n",
      "Epoch 5/20 Step 275/1867 — Loss: 0.0359\n",
      "Epoch 5/20 Step 300/1867 — Loss: 0.0599\n",
      "Epoch 5/20 Step 325/1867 — Loss: 0.1393\n",
      "Epoch 5/20 Step 350/1867 — Loss: 0.1562\n",
      "Epoch 5/20 Step 375/1867 — Loss: 0.2271\n",
      "Epoch 5/20 Step 400/1867 — Loss: 0.0606\n",
      "Epoch 5/20 Step 425/1867 — Loss: 0.0920\n",
      "Epoch 5/20 Step 450/1867 — Loss: 0.1095\n",
      "Epoch 5/20 Step 475/1867 — Loss: 0.2489\n",
      "Epoch 5/20 Step 500/1867 — Loss: 0.1364\n",
      "Epoch 5/20 Step 525/1867 — Loss: 0.1565\n",
      "Epoch 5/20 Step 550/1867 — Loss: 0.0761\n",
      "Epoch 5/20 Step 575/1867 — Loss: 0.1056\n",
      "Epoch 5/20 Step 600/1867 — Loss: 0.0473\n",
      "Epoch 5/20 Step 625/1867 — Loss: 0.0627\n",
      "Epoch 5/20 Step 650/1867 — Loss: 0.0483\n",
      "Epoch 5/20 Step 675/1867 — Loss: 0.0703\n",
      "Epoch 5/20 Step 700/1867 — Loss: 0.0453\n",
      "Epoch 5/20 Step 725/1867 — Loss: 0.0766\n",
      "Epoch 5/20 Step 750/1867 — Loss: 0.2103\n",
      "Epoch 5/20 Step 775/1867 — Loss: 0.1003\n",
      "Epoch 5/20 Step 800/1867 — Loss: 0.1366\n",
      "Epoch 5/20 Step 825/1867 — Loss: 0.0615\n",
      "Epoch 5/20 Step 850/1867 — Loss: 0.1062\n",
      "Epoch 5/20 Step 875/1867 — Loss: 0.0717\n",
      "Epoch 5/20 Step 900/1867 — Loss: 0.1592\n",
      "Epoch 5/20 Step 925/1867 — Loss: 0.3188\n",
      "Epoch 5/20 Step 950/1867 — Loss: 0.2003\n",
      "Epoch 5/20 Step 975/1867 — Loss: 0.0957\n",
      "Epoch 5/20 Step 1000/1867 — Loss: 0.1534\n",
      "Epoch 5/20 Step 1025/1867 — Loss: 0.1292\n",
      "Epoch 5/20 Step 1050/1867 — Loss: 0.1123\n",
      "Epoch 5/20 Step 1075/1867 — Loss: 0.0723\n",
      "Epoch 5/20 Step 1100/1867 — Loss: 0.2473\n",
      "Epoch 5/20 Step 1125/1867 — Loss: 0.0744\n",
      "Epoch 5/20 Step 1150/1867 — Loss: 0.0727\n",
      "Epoch 5/20 Step 1175/1867 — Loss: 0.0436\n",
      "Epoch 5/20 Step 1200/1867 — Loss: 0.1668\n",
      "Epoch 5/20 Step 1225/1867 — Loss: 0.0357\n",
      "Epoch 5/20 Step 1250/1867 — Loss: 0.0379\n",
      "Epoch 5/20 Step 1275/1867 — Loss: 0.1082\n",
      "Epoch 5/20 Step 1300/1867 — Loss: 0.0825\n",
      "Epoch 5/20 Step 1325/1867 — Loss: 0.0494\n",
      "Epoch 5/20 Step 1350/1867 — Loss: 0.0290\n",
      "Epoch 5/20 Step 1375/1867 — Loss: 0.1072\n",
      "Epoch 5/20 Step 1400/1867 — Loss: 0.0516\n",
      "Epoch 5/20 Step 1425/1867 — Loss: 0.0552\n",
      "Epoch 5/20 Step 1450/1867 — Loss: 0.2283\n",
      "Epoch 5/20 Step 1475/1867 — Loss: 0.1634\n",
      "Epoch 5/20 Step 1500/1867 — Loss: 0.1566\n",
      "Epoch 5/20 Step 1525/1867 — Loss: 0.0468\n",
      "Epoch 5/20 Step 1550/1867 — Loss: 0.1462\n",
      "Epoch 5/20 Step 1575/1867 — Loss: 0.1031\n",
      "Epoch 5/20 Step 1600/1867 — Loss: 0.0887\n",
      "Epoch 5/20 Step 1625/1867 — Loss: 0.1065\n",
      "Epoch 5/20 Step 1650/1867 — Loss: 0.0891\n",
      "Epoch 5/20 Step 1675/1867 — Loss: 0.1826\n",
      "Epoch 5/20 Step 1700/1867 — Loss: 0.1024\n",
      "Epoch 5/20 Step 1725/1867 — Loss: 0.1725\n",
      "Epoch 5/20 Step 1750/1867 — Loss: 0.0539\n",
      "Epoch 5/20 Step 1775/1867 — Loss: 0.0529\n",
      "Epoch 5/20 Step 1800/1867 — Loss: 0.0736\n",
      "Epoch 5/20 Step 1825/1867 — Loss: 0.2078\n",
      "Epoch 5/20 Step 1850/1867 — Loss: 0.1459\n",
      "Epoch 6/20 Step 0/1867 — Loss: 0.0967\n",
      "Epoch 6/20 Step 25/1867 — Loss: 0.0901\n",
      "Epoch 6/20 Step 50/1867 — Loss: 0.0508\n",
      "Epoch 6/20 Step 75/1867 — Loss: 0.0927\n",
      "Epoch 6/20 Step 100/1867 — Loss: 0.0901\n",
      "Epoch 6/20 Step 125/1867 — Loss: 0.1308\n",
      "Epoch 6/20 Step 150/1867 — Loss: 0.1508\n",
      "Epoch 6/20 Step 175/1867 — Loss: 0.0436\n",
      "Epoch 6/20 Step 200/1867 — Loss: 0.1006\n",
      "Epoch 6/20 Step 225/1867 — Loss: 0.0597\n",
      "Epoch 6/20 Step 250/1867 — Loss: 0.0760\n",
      "Epoch 6/20 Step 275/1867 — Loss: 0.0772\n",
      "Epoch 6/20 Step 300/1867 — Loss: 0.1065\n",
      "Epoch 6/20 Step 325/1867 — Loss: 0.1018\n",
      "Epoch 6/20 Step 350/1867 — Loss: 0.0477\n",
      "Epoch 6/20 Step 375/1867 — Loss: 0.0409\n",
      "Epoch 6/20 Step 400/1867 — Loss: 0.1173\n",
      "Epoch 6/20 Step 425/1867 — Loss: 0.2611\n",
      "Epoch 6/20 Step 450/1867 — Loss: 0.0482\n",
      "Epoch 6/20 Step 475/1867 — Loss: 0.1448\n",
      "Epoch 6/20 Step 500/1867 — Loss: 0.0469\n",
      "Epoch 6/20 Step 525/1867 — Loss: 0.0469\n",
      "Epoch 6/20 Step 550/1867 — Loss: 0.1693\n",
      "Epoch 6/20 Step 575/1867 — Loss: 0.0576\n",
      "Epoch 6/20 Step 600/1867 — Loss: 0.1073\n",
      "Epoch 6/20 Step 625/1867 — Loss: 0.0430\n",
      "Epoch 6/20 Step 650/1867 — Loss: 0.1763\n",
      "Epoch 6/20 Step 675/1867 — Loss: 0.1119\n",
      "Epoch 6/20 Step 700/1867 — Loss: 0.0953\n",
      "Epoch 6/20 Step 725/1867 — Loss: 0.0364\n",
      "Epoch 6/20 Step 750/1867 — Loss: 0.0271\n",
      "Epoch 6/20 Step 775/1867 — Loss: 0.1029\n",
      "Epoch 6/20 Step 800/1867 — Loss: 0.1334\n",
      "Epoch 6/20 Step 825/1867 — Loss: 0.2958\n",
      "Epoch 6/20 Step 850/1867 — Loss: 0.1459\n",
      "Epoch 6/20 Step 875/1867 — Loss: 0.0580\n",
      "Epoch 6/20 Step 900/1867 — Loss: 0.1044\n",
      "Epoch 6/20 Step 925/1867 — Loss: 0.1196\n",
      "Epoch 6/20 Step 950/1867 — Loss: 0.1331\n",
      "Epoch 6/20 Step 975/1867 — Loss: 0.1082\n",
      "Epoch 6/20 Step 1000/1867 — Loss: 0.1081\n",
      "Epoch 6/20 Step 1025/1867 — Loss: 0.0635\n",
      "Epoch 6/20 Step 1050/1867 — Loss: 0.0842\n",
      "Epoch 6/20 Step 1075/1867 — Loss: 0.0837\n",
      "Epoch 6/20 Step 1100/1867 — Loss: 0.0899\n",
      "Epoch 6/20 Step 1125/1867 — Loss: 0.1313\n",
      "Epoch 6/20 Step 1150/1867 — Loss: 0.1204\n",
      "Epoch 6/20 Step 1175/1867 — Loss: 0.0771\n",
      "Epoch 6/20 Step 1200/1867 — Loss: 0.1691\n",
      "Epoch 6/20 Step 1225/1867 — Loss: 0.1689\n",
      "Epoch 6/20 Step 1250/1867 — Loss: 0.3125\n",
      "Epoch 6/20 Step 1275/1867 — Loss: 0.1446\n",
      "Epoch 6/20 Step 1300/1867 — Loss: 0.0380\n",
      "Epoch 6/20 Step 1325/1867 — Loss: 0.1645\n",
      "Epoch 6/20 Step 1350/1867 — Loss: 0.1337\n",
      "Epoch 6/20 Step 1375/1867 — Loss: 0.1515\n",
      "Epoch 6/20 Step 1400/1867 — Loss: 0.0307\n",
      "Epoch 6/20 Step 1425/1867 — Loss: 0.0893\n",
      "Epoch 6/20 Step 1450/1867 — Loss: 0.0780\n",
      "Epoch 6/20 Step 1475/1867 — Loss: 0.0959\n",
      "Epoch 6/20 Step 1500/1867 — Loss: 0.0597\n",
      "Epoch 6/20 Step 1525/1867 — Loss: 0.1159\n",
      "Epoch 6/20 Step 1550/1867 — Loss: 0.0492\n",
      "Epoch 6/20 Step 1575/1867 — Loss: 0.1320\n",
      "Epoch 6/20 Step 1600/1867 — Loss: 0.1751\n",
      "Epoch 6/20 Step 1625/1867 — Loss: 0.0579\n",
      "Epoch 6/20 Step 1650/1867 — Loss: 0.0827\n",
      "Epoch 6/20 Step 1675/1867 — Loss: 0.0907\n",
      "Epoch 6/20 Step 1700/1867 — Loss: 0.1499\n",
      "Epoch 6/20 Step 1725/1867 — Loss: 0.0971\n",
      "Epoch 6/20 Step 1750/1867 — Loss: 0.0474\n",
      "Epoch 6/20 Step 1775/1867 — Loss: 0.0486\n",
      "Epoch 6/20 Step 1800/1867 — Loss: 0.0795\n",
      "Epoch 6/20 Step 1825/1867 — Loss: 0.1240\n",
      "Epoch 6/20 Step 1850/1867 — Loss: 0.1222\n",
      "Epoch 7/20 Step 0/1867 — Loss: 0.0895\n",
      "Epoch 7/20 Step 25/1867 — Loss: 0.0441\n",
      "Epoch 7/20 Step 50/1867 — Loss: 0.0777\n",
      "Epoch 7/20 Step 75/1867 — Loss: 0.1291\n",
      "Epoch 7/20 Step 100/1867 — Loss: 0.0425\n",
      "Epoch 7/20 Step 125/1867 — Loss: 0.0959\n",
      "Epoch 7/20 Step 150/1867 — Loss: 0.0895\n",
      "Epoch 7/20 Step 175/1867 — Loss: 0.1009\n",
      "Epoch 7/20 Step 200/1867 — Loss: 0.0549\n",
      "Epoch 7/20 Step 225/1867 — Loss: 0.1065\n",
      "Epoch 7/20 Step 250/1867 — Loss: 0.0881\n",
      "Epoch 7/20 Step 275/1867 — Loss: 0.0817\n",
      "Epoch 7/20 Step 300/1867 — Loss: 0.0657\n",
      "Epoch 7/20 Step 325/1867 — Loss: 0.0394\n",
      "Epoch 7/20 Step 350/1867 — Loss: 0.1108\n",
      "Epoch 7/20 Step 375/1867 — Loss: 0.1028\n",
      "Epoch 7/20 Step 400/1867 — Loss: 0.0567\n",
      "Epoch 7/20 Step 425/1867 — Loss: 0.0530\n",
      "Epoch 7/20 Step 450/1867 — Loss: 0.0637\n",
      "Epoch 7/20 Step 475/1867 — Loss: 0.0450\n",
      "Epoch 7/20 Step 500/1867 — Loss: 0.0689\n",
      "Epoch 7/20 Step 525/1867 — Loss: 0.0990\n",
      "Epoch 7/20 Step 550/1867 — Loss: 0.0937\n",
      "Epoch 7/20 Step 575/1867 — Loss: 0.0470\n",
      "Epoch 7/20 Step 600/1867 — Loss: 0.0581\n",
      "Epoch 7/20 Step 625/1867 — Loss: 0.0499\n",
      "Epoch 7/20 Step 650/1867 — Loss: 0.1072\n",
      "Epoch 7/20 Step 675/1867 — Loss: 0.0728\n",
      "Epoch 7/20 Step 700/1867 — Loss: 0.0460\n",
      "Epoch 7/20 Step 725/1867 — Loss: 0.0513\n",
      "Epoch 7/20 Step 750/1867 — Loss: 0.0548\n",
      "Epoch 7/20 Step 775/1867 — Loss: 0.0618\n",
      "Epoch 7/20 Step 800/1867 — Loss: 0.0903\n",
      "Epoch 7/20 Step 825/1867 — Loss: 0.0301\n",
      "Epoch 7/20 Step 850/1867 — Loss: 0.0700\n",
      "Epoch 7/20 Step 875/1867 — Loss: 0.1650\n",
      "Epoch 7/20 Step 900/1867 — Loss: 0.1381\n",
      "Epoch 7/20 Step 925/1867 — Loss: 0.1163\n",
      "Epoch 7/20 Step 950/1867 — Loss: 0.0537\n",
      "Epoch 7/20 Step 975/1867 — Loss: 0.0991\n",
      "Epoch 7/20 Step 1000/1867 — Loss: 0.1205\n",
      "Epoch 7/20 Step 1025/1867 — Loss: 0.0513\n",
      "Epoch 7/20 Step 1050/1867 — Loss: 0.0703\n",
      "Epoch 7/20 Step 1075/1867 — Loss: 0.0325\n",
      "Epoch 7/20 Step 1100/1867 — Loss: 0.0311\n",
      "Epoch 7/20 Step 1125/1867 — Loss: 0.1208\n",
      "Epoch 7/20 Step 1150/1867 — Loss: 0.0263\n",
      "Epoch 7/20 Step 1175/1867 — Loss: 0.0813\n",
      "Epoch 7/20 Step 1200/1867 — Loss: 0.0768\n",
      "Epoch 7/20 Step 1225/1867 — Loss: 0.0632\n",
      "Epoch 7/20 Step 1250/1867 — Loss: 0.0910\n",
      "Epoch 7/20 Step 1275/1867 — Loss: 0.0612\n",
      "Epoch 7/20 Step 1300/1867 — Loss: 0.0713\n",
      "Epoch 7/20 Step 1325/1867 — Loss: 0.0504\n",
      "Epoch 7/20 Step 1350/1867 — Loss: 0.0611\n",
      "Epoch 7/20 Step 1375/1867 — Loss: 0.0477\n",
      "Epoch 7/20 Step 1400/1867 — Loss: 0.1020\n",
      "Epoch 7/20 Step 1425/1867 — Loss: 0.0607\n",
      "Epoch 7/20 Step 1450/1867 — Loss: 0.0878\n",
      "Epoch 7/20 Step 1475/1867 — Loss: 0.1101\n",
      "Epoch 7/20 Step 1500/1867 — Loss: 0.1585\n",
      "Epoch 7/20 Step 1525/1867 — Loss: 0.1135\n",
      "Epoch 7/20 Step 1550/1867 — Loss: 0.1113\n",
      "Epoch 7/20 Step 1575/1867 — Loss: 0.1310\n",
      "Epoch 7/20 Step 1600/1867 — Loss: 0.0632\n",
      "Epoch 7/20 Step 1625/1867 — Loss: 0.0593\n",
      "Epoch 7/20 Step 1650/1867 — Loss: 0.0691\n",
      "Epoch 7/20 Step 1675/1867 — Loss: 0.0708\n",
      "Epoch 7/20 Step 1700/1867 — Loss: 0.0445\n",
      "Epoch 7/20 Step 1725/1867 — Loss: 0.0434\n",
      "Epoch 7/20 Step 1750/1867 — Loss: 0.1035\n",
      "Epoch 7/20 Step 1775/1867 — Loss: 0.0357\n",
      "Epoch 7/20 Step 1800/1867 — Loss: 0.0818\n",
      "Epoch 7/20 Step 1825/1867 — Loss: 0.0574\n",
      "Epoch 7/20 Step 1850/1867 — Loss: 0.0633\n",
      "Epoch 8/20 Step 0/1867 — Loss: 0.0579\n",
      "Epoch 8/20 Step 25/1867 — Loss: 0.1225\n",
      "Epoch 8/20 Step 50/1867 — Loss: 0.0508\n",
      "Epoch 8/20 Step 75/1867 — Loss: 0.1162\n",
      "Epoch 8/20 Step 100/1867 — Loss: 0.0744\n",
      "Epoch 8/20 Step 125/1867 — Loss: 0.1776\n",
      "Epoch 8/20 Step 150/1867 — Loss: 0.0688\n",
      "Epoch 8/20 Step 175/1867 — Loss: 0.0327\n",
      "Epoch 8/20 Step 200/1867 — Loss: 0.0338\n",
      "Epoch 8/20 Step 225/1867 — Loss: 0.0693\n",
      "Epoch 8/20 Step 250/1867 — Loss: 0.1081\n",
      "Epoch 8/20 Step 275/1867 — Loss: 0.0714\n",
      "Epoch 8/20 Step 300/1867 — Loss: 0.0666\n",
      "Epoch 8/20 Step 325/1867 — Loss: 0.0570\n",
      "Epoch 8/20 Step 350/1867 — Loss: 0.0460\n",
      "Epoch 8/20 Step 375/1867 — Loss: 0.0892\n",
      "Epoch 8/20 Step 400/1867 — Loss: 0.1145\n",
      "Epoch 8/20 Step 425/1867 — Loss: 0.0975\n",
      "Epoch 8/20 Step 450/1867 — Loss: 0.0489\n",
      "Epoch 8/20 Step 475/1867 — Loss: 0.1055\n",
      "Epoch 8/20 Step 500/1867 — Loss: 0.1013\n",
      "Epoch 8/20 Step 525/1867 — Loss: 0.1044\n",
      "Epoch 8/20 Step 550/1867 — Loss: 0.1270\n",
      "Epoch 8/20 Step 575/1867 — Loss: 0.2205\n",
      "Epoch 8/20 Step 600/1867 — Loss: 0.0526\n",
      "Epoch 8/20 Step 625/1867 — Loss: 0.0705\n",
      "Epoch 8/20 Step 650/1867 — Loss: 0.0545\n",
      "Epoch 8/20 Step 675/1867 — Loss: 0.0558\n",
      "Epoch 8/20 Step 700/1867 — Loss: 0.0678\n",
      "Epoch 8/20 Step 725/1867 — Loss: 0.1022\n",
      "Epoch 8/20 Step 750/1867 — Loss: 0.0364\n",
      "Epoch 8/20 Step 775/1867 — Loss: 0.0509\n",
      "Epoch 8/20 Step 800/1867 — Loss: 0.0576\n",
      "Epoch 8/20 Step 825/1867 — Loss: 0.0787\n",
      "Epoch 8/20 Step 850/1867 — Loss: 0.0750\n",
      "Epoch 8/20 Step 875/1867 — Loss: 0.0875\n",
      "Epoch 8/20 Step 900/1867 — Loss: 0.0671\n",
      "Epoch 8/20 Step 925/1867 — Loss: 0.1122\n",
      "Epoch 8/20 Step 950/1867 — Loss: 0.0455\n",
      "Epoch 8/20 Step 975/1867 — Loss: 0.0476\n",
      "Epoch 8/20 Step 1000/1867 — Loss: 0.0765\n",
      "Epoch 8/20 Step 1025/1867 — Loss: 0.0705\n",
      "Epoch 8/20 Step 1050/1867 — Loss: 0.0560\n",
      "Epoch 8/20 Step 1075/1867 — Loss: 0.0246\n",
      "Epoch 8/20 Step 1100/1867 — Loss: 0.0612\n",
      "Epoch 8/20 Step 1125/1867 — Loss: 0.0676\n",
      "Epoch 8/20 Step 1150/1867 — Loss: 0.0518\n",
      "Epoch 8/20 Step 1175/1867 — Loss: 0.1182\n",
      "Epoch 8/20 Step 1200/1867 — Loss: 0.0654\n",
      "Epoch 8/20 Step 1225/1867 — Loss: 0.0845\n",
      "Epoch 8/20 Step 1250/1867 — Loss: 0.0692\n",
      "Epoch 8/20 Step 1275/1867 — Loss: 0.1352\n",
      "Epoch 8/20 Step 1300/1867 — Loss: 0.0473\n",
      "Epoch 8/20 Step 1325/1867 — Loss: 0.1101\n",
      "Epoch 8/20 Step 1350/1867 — Loss: 0.0620\n",
      "Epoch 8/20 Step 1375/1867 — Loss: 0.0344\n",
      "Epoch 8/20 Step 1400/1867 — Loss: 0.0963\n",
      "Epoch 8/20 Step 1425/1867 — Loss: 0.0421\n",
      "Epoch 8/20 Step 1450/1867 — Loss: 0.0866\n",
      "Epoch 8/20 Step 1475/1867 — Loss: 0.0797\n",
      "Epoch 8/20 Step 1500/1867 — Loss: 0.1098\n",
      "Epoch 8/20 Step 1525/1867 — Loss: 0.0551\n",
      "Epoch 8/20 Step 1550/1867 — Loss: 0.0830\n",
      "Epoch 8/20 Step 1575/1867 — Loss: 0.0673\n",
      "Epoch 8/20 Step 1600/1867 — Loss: 0.0772\n",
      "Epoch 8/20 Step 1625/1867 — Loss: 0.0749\n",
      "Epoch 8/20 Step 1650/1867 — Loss: 0.0818\n",
      "Epoch 8/20 Step 1675/1867 — Loss: 0.0312\n",
      "Epoch 8/20 Step 1700/1867 — Loss: 0.0729\n",
      "Epoch 8/20 Step 1725/1867 — Loss: 0.0658\n",
      "Epoch 8/20 Step 1750/1867 — Loss: 0.0600\n",
      "Epoch 8/20 Step 1775/1867 — Loss: 0.0440\n",
      "Epoch 8/20 Step 1800/1867 — Loss: 0.1185\n",
      "Epoch 8/20 Step 1825/1867 — Loss: 0.0847\n",
      "Epoch 8/20 Step 1850/1867 — Loss: 0.0365\n",
      "Epoch 9/20 Step 0/1867 — Loss: 0.0607\n",
      "Epoch 9/20 Step 25/1867 — Loss: 0.0751\n",
      "Epoch 9/20 Step 50/1867 — Loss: 0.0404\n",
      "Epoch 9/20 Step 75/1867 — Loss: 0.0459\n",
      "Epoch 9/20 Step 100/1867 — Loss: 0.1125\n",
      "Epoch 9/20 Step 125/1867 — Loss: 0.0715\n",
      "Epoch 9/20 Step 150/1867 — Loss: 0.0382\n",
      "Epoch 9/20 Step 175/1867 — Loss: 0.0712\n",
      "Epoch 9/20 Step 200/1867 — Loss: 0.0859\n",
      "Epoch 9/20 Step 225/1867 — Loss: 0.0658\n",
      "Epoch 9/20 Step 250/1867 — Loss: 0.0584\n",
      "Epoch 9/20 Step 275/1867 — Loss: 0.0624\n",
      "Epoch 9/20 Step 300/1867 — Loss: 0.1227\n",
      "Epoch 9/20 Step 325/1867 — Loss: 0.0794\n",
      "Epoch 9/20 Step 350/1867 — Loss: 0.0631\n",
      "Epoch 9/20 Step 375/1867 — Loss: 0.0762\n",
      "Epoch 9/20 Step 400/1867 — Loss: 0.0401\n",
      "Epoch 9/20 Step 425/1867 — Loss: 0.0650\n",
      "Epoch 9/20 Step 450/1867 — Loss: 0.0527\n",
      "Epoch 9/20 Step 475/1867 — Loss: 0.0653\n",
      "Epoch 9/20 Step 500/1867 — Loss: 0.0584\n",
      "Epoch 9/20 Step 525/1867 — Loss: 0.1016\n",
      "Epoch 9/20 Step 550/1867 — Loss: 0.0781\n",
      "Epoch 9/20 Step 575/1867 — Loss: 0.1032\n",
      "Epoch 9/20 Step 600/1867 — Loss: 0.0466\n",
      "Epoch 9/20 Step 625/1867 — Loss: 0.0654\n",
      "Epoch 9/20 Step 650/1867 — Loss: 0.0848\n",
      "Epoch 9/20 Step 675/1867 — Loss: 0.0795\n",
      "Epoch 9/20 Step 700/1867 — Loss: 0.0717\n",
      "Epoch 9/20 Step 725/1867 — Loss: 0.1663\n",
      "Epoch 9/20 Step 750/1867 — Loss: 0.0843\n",
      "Epoch 9/20 Step 775/1867 — Loss: 0.0869\n",
      "Epoch 9/20 Step 800/1867 — Loss: 0.1104\n",
      "Epoch 9/20 Step 825/1867 — Loss: 0.0495\n",
      "Epoch 9/20 Step 850/1867 — Loss: 0.0579\n",
      "Epoch 9/20 Step 875/1867 — Loss: 0.0511\n",
      "Epoch 9/20 Step 900/1867 — Loss: 0.0582\n",
      "Epoch 9/20 Step 925/1867 — Loss: 0.0778\n",
      "Epoch 9/20 Step 950/1867 — Loss: 0.0519\n",
      "Epoch 9/20 Step 975/1867 — Loss: 0.0546\n",
      "Epoch 9/20 Step 1000/1867 — Loss: 0.0354\n",
      "Epoch 9/20 Step 1025/1867 — Loss: 0.0291\n",
      "Epoch 9/20 Step 1050/1867 — Loss: 0.0889\n",
      "Epoch 9/20 Step 1075/1867 — Loss: 0.0605\n",
      "Epoch 9/20 Step 1100/1867 — Loss: 0.0315\n",
      "Epoch 9/20 Step 1125/1867 — Loss: 0.0809\n",
      "Epoch 9/20 Step 1150/1867 — Loss: 0.0426\n",
      "Epoch 9/20 Step 1175/1867 — Loss: 0.0647\n",
      "Epoch 9/20 Step 1200/1867 — Loss: 0.0683\n",
      "Epoch 9/20 Step 1225/1867 — Loss: 0.0753\n",
      "Epoch 9/20 Step 1250/1867 — Loss: 0.0399\n",
      "Epoch 9/20 Step 1275/1867 — Loss: 0.0510\n",
      "Epoch 9/20 Step 1300/1867 — Loss: 0.1031\n",
      "Epoch 9/20 Step 1325/1867 — Loss: 0.0851\n",
      "Epoch 9/20 Step 1350/1867 — Loss: 0.0566\n",
      "Epoch 9/20 Step 1375/1867 — Loss: 0.0540\n",
      "Epoch 9/20 Step 1400/1867 — Loss: 0.0543\n",
      "Epoch 9/20 Step 1425/1867 — Loss: 0.0579\n",
      "Epoch 9/20 Step 1450/1867 — Loss: 0.0890\n",
      "Epoch 9/20 Step 1475/1867 — Loss: 0.0478\n",
      "Epoch 9/20 Step 1500/1867 — Loss: 0.0765\n",
      "Epoch 9/20 Step 1525/1867 — Loss: 0.0645\n",
      "Epoch 9/20 Step 1550/1867 — Loss: 0.0952\n",
      "Epoch 9/20 Step 1575/1867 — Loss: 0.1182\n",
      "Epoch 9/20 Step 1600/1867 — Loss: 0.1138\n",
      "Epoch 9/20 Step 1625/1867 — Loss: 0.0644\n",
      "Epoch 9/20 Step 1650/1867 — Loss: 0.0496\n",
      "Epoch 9/20 Step 1675/1867 — Loss: 0.0336\n",
      "Epoch 9/20 Step 1700/1867 — Loss: 0.0829\n",
      "Epoch 9/20 Step 1725/1867 — Loss: 0.0875\n",
      "Epoch 9/20 Step 1750/1867 — Loss: 0.0377\n",
      "Epoch 9/20 Step 1775/1867 — Loss: 0.0709\n",
      "Epoch 9/20 Step 1800/1867 — Loss: 0.0668\n",
      "Epoch 9/20 Step 1825/1867 — Loss: 0.0999\n",
      "Epoch 9/20 Step 1850/1867 — Loss: 0.0598\n",
      "Epoch 10/20 Step 0/1867 — Loss: 0.0769\n",
      "Epoch 10/20 Step 25/1867 — Loss: 0.0610\n",
      "Epoch 10/20 Step 50/1867 — Loss: 0.0569\n",
      "Epoch 10/20 Step 75/1867 — Loss: 0.0528\n",
      "Epoch 10/20 Step 100/1867 — Loss: 0.0702\n",
      "Epoch 10/20 Step 125/1867 — Loss: 0.0270\n",
      "Epoch 10/20 Step 150/1867 — Loss: 0.0649\n",
      "Epoch 10/20 Step 175/1867 — Loss: 0.0780\n",
      "Epoch 10/20 Step 200/1867 — Loss: 0.0616\n",
      "Epoch 10/20 Step 225/1867 — Loss: 0.0437\n",
      "Epoch 10/20 Step 250/1867 — Loss: 0.0689\n",
      "Epoch 10/20 Step 275/1867 — Loss: 0.0608\n",
      "Epoch 10/20 Step 300/1867 — Loss: 0.0313\n",
      "Epoch 10/20 Step 325/1867 — Loss: 0.0826\n",
      "Epoch 10/20 Step 350/1867 — Loss: 0.0705\n",
      "Epoch 10/20 Step 375/1867 — Loss: 0.0485\n",
      "Epoch 10/20 Step 400/1867 — Loss: 0.0543\n",
      "Epoch 10/20 Step 425/1867 — Loss: 0.0843\n",
      "Epoch 10/20 Step 450/1867 — Loss: 0.0581\n",
      "Epoch 10/20 Step 475/1867 — Loss: 0.0317\n",
      "Epoch 10/20 Step 500/1867 — Loss: 0.0798\n",
      "Epoch 10/20 Step 525/1867 — Loss: 0.0577\n",
      "Epoch 10/20 Step 550/1867 — Loss: 0.0560\n",
      "Epoch 10/20 Step 575/1867 — Loss: 0.0528\n",
      "Epoch 10/20 Step 600/1867 — Loss: 0.0774\n",
      "Epoch 10/20 Step 625/1867 — Loss: 0.0643\n",
      "Epoch 10/20 Step 650/1867 — Loss: 0.0406\n",
      "Epoch 10/20 Step 675/1867 — Loss: 0.0548\n",
      "Epoch 10/20 Step 700/1867 — Loss: 0.0428\n",
      "Epoch 10/20 Step 725/1867 — Loss: 0.0507\n",
      "Epoch 10/20 Step 750/1867 — Loss: 0.0636\n",
      "Epoch 10/20 Step 775/1867 — Loss: 0.0940\n",
      "Epoch 10/20 Step 800/1867 — Loss: 0.0567\n",
      "Epoch 10/20 Step 825/1867 — Loss: 0.1016\n",
      "Epoch 10/20 Step 850/1867 — Loss: 0.0578\n",
      "Epoch 10/20 Step 875/1867 — Loss: 0.0931\n",
      "Epoch 10/20 Step 900/1867 — Loss: 0.0673\n",
      "Epoch 10/20 Step 925/1867 — Loss: 0.0589\n",
      "Epoch 10/20 Step 950/1867 — Loss: 0.0665\n",
      "Epoch 10/20 Step 975/1867 — Loss: 0.0472\n",
      "Epoch 10/20 Step 1000/1867 — Loss: 0.0437\n",
      "Epoch 10/20 Step 1025/1867 — Loss: 0.0503\n",
      "Epoch 10/20 Step 1050/1867 — Loss: 0.0483\n",
      "Epoch 10/20 Step 1075/1867 — Loss: 0.0834\n",
      "Epoch 10/20 Step 1100/1867 — Loss: 0.0292\n",
      "Epoch 10/20 Step 1125/1867 — Loss: 0.1177\n",
      "Epoch 10/20 Step 1150/1867 — Loss: 0.0576\n",
      "Epoch 10/20 Step 1175/1867 — Loss: 0.0328\n",
      "Epoch 10/20 Step 1200/1867 — Loss: 0.0689\n",
      "Epoch 10/20 Step 1225/1867 — Loss: 0.0873\n",
      "Epoch 10/20 Step 1250/1867 — Loss: 0.0772\n",
      "Epoch 10/20 Step 1275/1867 — Loss: 0.0873\n",
      "Epoch 10/20 Step 1300/1867 — Loss: 0.0339\n",
      "Epoch 10/20 Step 1325/1867 — Loss: 0.0777\n",
      "Epoch 10/20 Step 1350/1867 — Loss: 0.0394\n",
      "Epoch 10/20 Step 1375/1867 — Loss: 0.0880\n",
      "Epoch 10/20 Step 1400/1867 — Loss: 0.0605\n",
      "Epoch 10/20 Step 1425/1867 — Loss: 0.0490\n",
      "Epoch 10/20 Step 1450/1867 — Loss: 0.0776\n",
      "Epoch 10/20 Step 1475/1867 — Loss: 0.0535\n",
      "Epoch 10/20 Step 1500/1867 — Loss: 0.0352\n",
      "Epoch 10/20 Step 1525/1867 — Loss: 0.0721\n",
      "Epoch 10/20 Step 1550/1867 — Loss: 0.1007\n",
      "Epoch 10/20 Step 1575/1867 — Loss: 0.0351\n",
      "Epoch 10/20 Step 1600/1867 — Loss: 0.0396\n",
      "Epoch 10/20 Step 1625/1867 — Loss: 0.0726\n",
      "Epoch 10/20 Step 1650/1867 — Loss: 0.0743\n",
      "Epoch 10/20 Step 1675/1867 — Loss: 0.0584\n",
      "Epoch 10/20 Step 1700/1867 — Loss: 0.0546\n",
      "Epoch 10/20 Step 1725/1867 — Loss: 0.0653\n",
      "Epoch 10/20 Step 1750/1867 — Loss: 0.0848\n",
      "Epoch 10/20 Step 1775/1867 — Loss: 0.0744\n",
      "Epoch 10/20 Step 1800/1867 — Loss: 0.0508\n",
      "Epoch 10/20 Step 1825/1867 — Loss: 0.0398\n",
      "Epoch 10/20 Step 1850/1867 — Loss: 0.0492\n",
      "Epoch 11/20 Step 0/1867 — Loss: 0.0418\n",
      "Epoch 11/20 Step 25/1867 — Loss: 0.0501\n",
      "Epoch 11/20 Step 50/1867 — Loss: 0.0509\n",
      "Epoch 11/20 Step 75/1867 — Loss: 0.0432\n",
      "Epoch 11/20 Step 100/1867 — Loss: 0.0589\n",
      "Epoch 11/20 Step 125/1867 — Loss: 0.0805\n",
      "Epoch 11/20 Step 150/1867 — Loss: 0.0650\n",
      "Epoch 11/20 Step 175/1867 — Loss: 0.0555\n",
      "Epoch 11/20 Step 200/1867 — Loss: 0.0607\n",
      "Epoch 11/20 Step 225/1867 — Loss: 0.0280\n",
      "Epoch 11/20 Step 250/1867 — Loss: 0.0844\n",
      "Epoch 11/20 Step 275/1867 — Loss: 0.0653\n",
      "Epoch 11/20 Step 300/1867 — Loss: 0.0509\n",
      "Epoch 11/20 Step 325/1867 — Loss: 0.0426\n",
      "Epoch 11/20 Step 350/1867 — Loss: 0.0584\n",
      "Epoch 11/20 Step 375/1867 — Loss: 0.0563\n",
      "Epoch 11/20 Step 400/1867 — Loss: 0.0608\n",
      "Epoch 11/20 Step 425/1867 — Loss: 0.0547\n",
      "Epoch 11/20 Step 450/1867 — Loss: 0.0449\n",
      "Epoch 11/20 Step 475/1867 — Loss: 0.0506\n",
      "Epoch 11/20 Step 500/1867 — Loss: 0.0350\n",
      "Epoch 11/20 Step 525/1867 — Loss: 0.0652\n",
      "Epoch 11/20 Step 550/1867 — Loss: 0.0921\n",
      "Epoch 11/20 Step 575/1867 — Loss: 0.0536\n",
      "Epoch 11/20 Step 600/1867 — Loss: 0.0321\n",
      "Epoch 11/20 Step 625/1867 — Loss: 0.0796\n",
      "Epoch 11/20 Step 650/1867 — Loss: 0.0685\n",
      "Epoch 11/20 Step 675/1867 — Loss: 0.0516\n",
      "Epoch 11/20 Step 700/1867 — Loss: 0.0422\n",
      "Epoch 11/20 Step 725/1867 — Loss: 0.0695\n",
      "Epoch 11/20 Step 750/1867 — Loss: 0.0568\n",
      "Epoch 11/20 Step 775/1867 — Loss: 0.0468\n",
      "Epoch 11/20 Step 800/1867 — Loss: 0.0346\n",
      "Epoch 11/20 Step 825/1867 — Loss: 0.0444\n",
      "Epoch 11/20 Step 850/1867 — Loss: 0.0552\n",
      "Epoch 11/20 Step 875/1867 — Loss: 0.0419\n",
      "Epoch 11/20 Step 900/1867 — Loss: 0.0257\n",
      "Epoch 11/20 Step 925/1867 — Loss: 0.0611\n",
      "Epoch 11/20 Step 950/1867 — Loss: 0.0758\n",
      "Epoch 11/20 Step 975/1867 — Loss: 0.0417\n",
      "Epoch 11/20 Step 1000/1867 — Loss: 0.0510\n",
      "Epoch 11/20 Step 1025/1867 — Loss: 0.0825\n",
      "Epoch 11/20 Step 1050/1867 — Loss: 0.0465\n",
      "Epoch 11/20 Step 1075/1867 — Loss: 0.1089\n",
      "Epoch 11/20 Step 1100/1867 — Loss: 0.0858\n",
      "Epoch 11/20 Step 1125/1867 — Loss: 0.0430\n",
      "Epoch 11/20 Step 1150/1867 — Loss: 0.0549\n",
      "Epoch 11/20 Step 1175/1867 — Loss: 0.0952\n",
      "Epoch 11/20 Step 1200/1867 — Loss: 0.0527\n",
      "Epoch 11/20 Step 1225/1867 — Loss: 0.0632\n",
      "Epoch 11/20 Step 1250/1867 — Loss: 0.0713\n",
      "Epoch 11/20 Step 1275/1867 — Loss: 0.0610\n",
      "Epoch 11/20 Step 1300/1867 — Loss: 0.0507\n",
      "Epoch 11/20 Step 1325/1867 — Loss: 0.0572\n",
      "Epoch 11/20 Step 1350/1867 — Loss: 0.0450\n",
      "Epoch 11/20 Step 1375/1867 — Loss: 0.0845\n",
      "Epoch 11/20 Step 1400/1867 — Loss: 0.0663\n",
      "Epoch 11/20 Step 1425/1867 — Loss: 0.0709\n",
      "Epoch 11/20 Step 1450/1867 — Loss: 0.0542\n",
      "Epoch 11/20 Step 1475/1867 — Loss: 0.0787\n",
      "Epoch 11/20 Step 1500/1867 — Loss: 0.0717\n",
      "Epoch 11/20 Step 1525/1867 — Loss: 0.0547\n",
      "Epoch 11/20 Step 1550/1867 — Loss: 0.0593\n",
      "Epoch 11/20 Step 1575/1867 — Loss: 0.0838\n",
      "Epoch 11/20 Step 1600/1867 — Loss: 0.0661\n",
      "Epoch 11/20 Step 1625/1867 — Loss: 0.0600\n",
      "Epoch 11/20 Step 1650/1867 — Loss: 0.0682\n",
      "Epoch 11/20 Step 1675/1867 — Loss: 0.0619\n",
      "Epoch 11/20 Step 1700/1867 — Loss: 0.0616\n",
      "Epoch 11/20 Step 1725/1867 — Loss: 0.0627\n",
      "Epoch 11/20 Step 1750/1867 — Loss: 0.0428\n",
      "Epoch 11/20 Step 1775/1867 — Loss: 0.0427\n",
      "Epoch 11/20 Step 1800/1867 — Loss: 0.0495\n",
      "Epoch 11/20 Step 1825/1867 — Loss: 0.0933\n",
      "Epoch 11/20 Step 1850/1867 — Loss: 0.0717\n",
      "Epoch 12/20 Step 0/1867 — Loss: 0.0577\n",
      "Epoch 12/20 Step 25/1867 — Loss: 0.0342\n",
      "Epoch 12/20 Step 50/1867 — Loss: 0.0599\n",
      "Epoch 12/20 Step 75/1867 — Loss: 0.0707\n",
      "Epoch 12/20 Step 100/1867 — Loss: 0.0401\n",
      "Epoch 12/20 Step 125/1867 — Loss: 0.0406\n",
      "Epoch 12/20 Step 150/1867 — Loss: 0.0891\n",
      "Epoch 12/20 Step 175/1867 — Loss: 0.0268\n",
      "Epoch 12/20 Step 200/1867 — Loss: 0.0541\n",
      "Epoch 12/20 Step 225/1867 — Loss: 0.0655\n",
      "Epoch 12/20 Step 250/1867 — Loss: 0.0574\n",
      "Epoch 12/20 Step 275/1867 — Loss: 0.0929\n",
      "Epoch 12/20 Step 300/1867 — Loss: 0.0651\n",
      "Epoch 12/20 Step 325/1867 — Loss: 0.0520\n",
      "Epoch 12/20 Step 350/1867 — Loss: 0.0572\n",
      "Epoch 12/20 Step 375/1867 — Loss: 0.0740\n",
      "Epoch 12/20 Step 400/1867 — Loss: 0.0744\n",
      "Epoch 12/20 Step 425/1867 — Loss: 0.0562\n",
      "Epoch 12/20 Step 450/1867 — Loss: 0.0806\n",
      "Epoch 12/20 Step 475/1867 — Loss: 0.0558\n",
      "Epoch 12/20 Step 500/1867 — Loss: 0.0471\n",
      "Epoch 12/20 Step 525/1867 — Loss: 0.0364\n",
      "Epoch 12/20 Step 550/1867 — Loss: 0.0609\n",
      "Epoch 12/20 Step 575/1867 — Loss: 0.0617\n",
      "Epoch 12/20 Step 600/1867 — Loss: 0.0606\n",
      "Epoch 12/20 Step 625/1867 — Loss: 0.0507\n",
      "Epoch 12/20 Step 650/1867 — Loss: 0.0490\n",
      "Epoch 12/20 Step 675/1867 — Loss: 0.0434\n",
      "Epoch 12/20 Step 700/1867 — Loss: 0.0336\n",
      "Epoch 12/20 Step 725/1867 — Loss: 0.0427\n",
      "Epoch 12/20 Step 750/1867 — Loss: 0.0633\n",
      "Epoch 12/20 Step 775/1867 — Loss: 0.0464\n",
      "Epoch 12/20 Step 800/1867 — Loss: 0.0610\n",
      "Epoch 12/20 Step 825/1867 — Loss: 0.0720\n",
      "Epoch 12/20 Step 850/1867 — Loss: 0.0472\n",
      "Epoch 12/20 Step 875/1867 — Loss: 0.0817\n",
      "Epoch 12/20 Step 900/1867 — Loss: 0.0638\n",
      "Epoch 12/20 Step 925/1867 — Loss: 0.0667\n",
      "Epoch 12/20 Step 950/1867 — Loss: 0.0696\n",
      "Epoch 12/20 Step 975/1867 — Loss: 0.0637\n",
      "Epoch 12/20 Step 1000/1867 — Loss: 0.0781\n",
      "Epoch 12/20 Step 1025/1867 — Loss: 0.0569\n",
      "Epoch 12/20 Step 1050/1867 — Loss: 0.0499\n",
      "Epoch 12/20 Step 1075/1867 — Loss: 0.0548\n",
      "Epoch 12/20 Step 1100/1867 — Loss: 0.0789\n",
      "Epoch 12/20 Step 1125/1867 — Loss: 0.0899\n",
      "Epoch 12/20 Step 1150/1867 — Loss: 0.0451\n",
      "Epoch 12/20 Step 1175/1867 — Loss: 0.0542\n",
      "Epoch 12/20 Step 1200/1867 — Loss: 0.0460\n",
      "Epoch 12/20 Step 1225/1867 — Loss: 0.0319\n",
      "Epoch 12/20 Step 1250/1867 — Loss: 0.0923\n",
      "Epoch 12/20 Step 1275/1867 — Loss: 0.0940\n",
      "Epoch 12/20 Step 1300/1867 — Loss: 0.0774\n",
      "Epoch 12/20 Step 1325/1867 — Loss: 0.0565\n",
      "Epoch 12/20 Step 1350/1867 — Loss: 0.0442\n",
      "Epoch 12/20 Step 1375/1867 — Loss: 0.0378\n",
      "Epoch 12/20 Step 1400/1867 — Loss: 0.0577\n",
      "Epoch 12/20 Step 1425/1867 — Loss: 0.0550\n",
      "Epoch 12/20 Step 1450/1867 — Loss: 0.0904\n",
      "Epoch 12/20 Step 1475/1867 — Loss: 0.0563\n",
      "Epoch 12/20 Step 1500/1867 — Loss: 0.0652\n",
      "Epoch 12/20 Step 1525/1867 — Loss: 0.0566\n",
      "Epoch 12/20 Step 1550/1867 — Loss: 0.1043\n",
      "Epoch 12/20 Step 1575/1867 — Loss: 0.0302\n",
      "Epoch 12/20 Step 1600/1867 — Loss: 0.0613\n",
      "Epoch 12/20 Step 1625/1867 — Loss: 0.0662\n",
      "Epoch 12/20 Step 1650/1867 — Loss: 0.0893\n",
      "Epoch 12/20 Step 1675/1867 — Loss: 0.0447\n",
      "Epoch 12/20 Step 1700/1867 — Loss: 0.0448\n",
      "Epoch 12/20 Step 1725/1867 — Loss: 0.0720\n",
      "Epoch 12/20 Step 1750/1867 — Loss: 0.0464\n",
      "Epoch 12/20 Step 1775/1867 — Loss: 0.0713\n",
      "Epoch 12/20 Step 1800/1867 — Loss: 0.0272\n",
      "Epoch 12/20 Step 1825/1867 — Loss: 0.0556\n",
      "Epoch 12/20 Step 1850/1867 — Loss: 0.0634\n",
      "Epoch 13/20 Step 0/1867 — Loss: 0.0762\n",
      "Epoch 13/20 Step 25/1867 — Loss: 0.0559\n",
      "Epoch 13/20 Step 50/1867 — Loss: 0.0515\n",
      "Epoch 13/20 Step 75/1867 — Loss: 0.0453\n",
      "Epoch 13/20 Step 100/1867 — Loss: 0.0343\n",
      "Epoch 13/20 Step 125/1867 — Loss: 0.0695\n",
      "Epoch 13/20 Step 150/1867 — Loss: 0.0686\n",
      "Epoch 13/20 Step 175/1867 — Loss: 0.0886\n",
      "Epoch 13/20 Step 200/1867 — Loss: 0.0267\n",
      "Epoch 13/20 Step 225/1867 — Loss: 0.0414\n",
      "Epoch 13/20 Step 250/1867 — Loss: 0.0516\n",
      "Epoch 13/20 Step 275/1867 — Loss: 0.0573\n",
      "Epoch 13/20 Step 300/1867 — Loss: 0.0394\n",
      "Epoch 13/20 Step 325/1867 — Loss: 0.0462\n",
      "Epoch 13/20 Step 350/1867 — Loss: 0.0495\n",
      "Epoch 13/20 Step 375/1867 — Loss: 0.0898\n",
      "Epoch 13/20 Step 400/1867 — Loss: 0.0546\n",
      "Epoch 13/20 Step 425/1867 — Loss: 0.0667\n",
      "Epoch 13/20 Step 450/1867 — Loss: 0.0478\n",
      "Epoch 13/20 Step 475/1867 — Loss: 0.0571\n",
      "Epoch 13/20 Step 500/1867 — Loss: 0.0493\n",
      "Epoch 13/20 Step 525/1867 — Loss: 0.0970\n",
      "Epoch 13/20 Step 550/1867 — Loss: 0.0410\n",
      "Epoch 13/20 Step 575/1867 — Loss: 0.0428\n",
      "Epoch 13/20 Step 600/1867 — Loss: 0.0345\n",
      "Epoch 13/20 Step 625/1867 — Loss: 0.0666\n",
      "Epoch 13/20 Step 650/1867 — Loss: 0.0406\n",
      "Epoch 13/20 Step 675/1867 — Loss: 0.0629\n",
      "Epoch 13/20 Step 700/1867 — Loss: 0.0366\n",
      "Epoch 13/20 Step 725/1867 — Loss: 0.0477\n",
      "Epoch 13/20 Step 750/1867 — Loss: 0.0317\n",
      "Epoch 13/20 Step 775/1867 — Loss: 0.0409\n",
      "Epoch 13/20 Step 800/1867 — Loss: 0.0656\n",
      "Epoch 13/20 Step 825/1867 — Loss: 0.0522\n",
      "Epoch 13/20 Step 850/1867 — Loss: 0.0655\n",
      "Epoch 13/20 Step 875/1867 — Loss: 0.0314\n",
      "Epoch 13/20 Step 900/1867 — Loss: 0.0334\n",
      "Epoch 13/20 Step 925/1867 — Loss: 0.0575\n",
      "Epoch 13/20 Step 950/1867 — Loss: 0.0606\n",
      "Epoch 13/20 Step 975/1867 — Loss: 0.0566\n",
      "Epoch 13/20 Step 1000/1867 — Loss: 0.0682\n",
      "Epoch 13/20 Step 1025/1867 — Loss: 0.0395\n",
      "Epoch 13/20 Step 1050/1867 — Loss: 0.0704\n",
      "Epoch 13/20 Step 1075/1867 — Loss: 0.0672\n",
      "Epoch 13/20 Step 1100/1867 — Loss: 0.0400\n",
      "Epoch 13/20 Step 1125/1867 — Loss: 0.0583\n",
      "Epoch 13/20 Step 1150/1867 — Loss: 0.0571\n",
      "Epoch 13/20 Step 1175/1867 — Loss: 0.0978\n",
      "Epoch 13/20 Step 1200/1867 — Loss: 0.0360\n",
      "Epoch 13/20 Step 1225/1867 — Loss: 0.0579\n",
      "Epoch 13/20 Step 1250/1867 — Loss: 0.0593\n",
      "Epoch 13/20 Step 1275/1867 — Loss: 0.0476\n",
      "Epoch 13/20 Step 1300/1867 — Loss: 0.0412\n",
      "Epoch 13/20 Step 1325/1867 — Loss: 0.0382\n",
      "Epoch 13/20 Step 1350/1867 — Loss: 0.0629\n",
      "Epoch 13/20 Step 1375/1867 — Loss: 0.0506\n",
      "Epoch 13/20 Step 1400/1867 — Loss: 0.0648\n",
      "Epoch 13/20 Step 1425/1867 — Loss: 0.0399\n",
      "Epoch 13/20 Step 1450/1867 — Loss: 0.0184\n",
      "Epoch 13/20 Step 1475/1867 — Loss: 0.0627\n",
      "Epoch 13/20 Step 1500/1867 — Loss: 0.0437\n",
      "Epoch 13/20 Step 1525/1867 — Loss: 0.0626\n",
      "Epoch 13/20 Step 1550/1867 — Loss: 0.0529\n",
      "Epoch 13/20 Step 1575/1867 — Loss: 0.0658\n",
      "Epoch 13/20 Step 1600/1867 — Loss: 0.0193\n",
      "Epoch 13/20 Step 1625/1867 — Loss: 0.0709\n",
      "Epoch 13/20 Step 1650/1867 — Loss: 0.0664\n",
      "Epoch 13/20 Step 1675/1867 — Loss: 0.0507\n",
      "Epoch 13/20 Step 1700/1867 — Loss: 0.0371\n",
      "Epoch 13/20 Step 1725/1867 — Loss: 0.0598\n",
      "Epoch 13/20 Step 1750/1867 — Loss: 0.0637\n",
      "Epoch 13/20 Step 1775/1867 — Loss: 0.0317\n",
      "Epoch 13/20 Step 1800/1867 — Loss: 0.0631\n",
      "Epoch 13/20 Step 1825/1867 — Loss: 0.0407\n",
      "Epoch 13/20 Step 1850/1867 — Loss: 0.0533\n",
      "Epoch 14/20 Step 0/1867 — Loss: 0.0370\n",
      "Epoch 14/20 Step 25/1867 — Loss: 0.0523\n",
      "Epoch 14/20 Step 50/1867 — Loss: 0.0760\n",
      "Epoch 14/20 Step 75/1867 — Loss: 0.0703\n",
      "Epoch 14/20 Step 100/1867 — Loss: 0.0426\n",
      "Epoch 14/20 Step 125/1867 — Loss: 0.0504\n",
      "Epoch 14/20 Step 150/1867 — Loss: 0.0437\n",
      "Epoch 14/20 Step 175/1867 — Loss: 0.0433\n",
      "Epoch 14/20 Step 200/1867 — Loss: 0.0706\n",
      "Epoch 14/20 Step 225/1867 — Loss: 0.0655\n",
      "Epoch 14/20 Step 250/1867 — Loss: 0.0525\n",
      "Epoch 14/20 Step 275/1867 — Loss: 0.0675\n",
      "Epoch 14/20 Step 300/1867 — Loss: 0.0346\n",
      "Epoch 14/20 Step 325/1867 — Loss: 0.0484\n",
      "Epoch 14/20 Step 350/1867 — Loss: 0.0421\n",
      "Epoch 14/20 Step 375/1867 — Loss: 0.0422\n",
      "Epoch 14/20 Step 400/1867 — Loss: 0.0538\n",
      "Epoch 14/20 Step 425/1867 — Loss: 0.0724\n",
      "Epoch 14/20 Step 450/1867 — Loss: 0.0357\n",
      "Epoch 14/20 Step 475/1867 — Loss: 0.0585\n",
      "Epoch 14/20 Step 500/1867 — Loss: 0.0574\n",
      "Epoch 14/20 Step 525/1867 — Loss: 0.0395\n",
      "Epoch 14/20 Step 550/1867 — Loss: 0.0541\n",
      "Epoch 14/20 Step 575/1867 — Loss: 0.0600\n",
      "Epoch 14/20 Step 600/1867 — Loss: 0.0623\n",
      "Epoch 14/20 Step 625/1867 — Loss: 0.0678\n",
      "Epoch 14/20 Step 650/1867 — Loss: 0.0442\n",
      "Epoch 14/20 Step 675/1867 — Loss: 0.0496\n",
      "Epoch 14/20 Step 700/1867 — Loss: 0.0642\n",
      "Epoch 14/20 Step 725/1867 — Loss: 0.0287\n",
      "Epoch 14/20 Step 750/1867 — Loss: 0.0522\n",
      "Epoch 14/20 Step 775/1867 — Loss: 0.0466\n",
      "Epoch 14/20 Step 800/1867 — Loss: 0.0450\n",
      "Epoch 14/20 Step 825/1867 — Loss: 0.0273\n",
      "Epoch 14/20 Step 850/1867 — Loss: 0.0618\n",
      "Epoch 14/20 Step 875/1867 — Loss: 0.0615\n",
      "Epoch 14/20 Step 900/1867 — Loss: 0.0656\n",
      "Epoch 14/20 Step 925/1867 — Loss: 0.1019\n",
      "Epoch 14/20 Step 950/1867 — Loss: 0.0725\n",
      "Epoch 14/20 Step 975/1867 — Loss: 0.0566\n",
      "Epoch 14/20 Step 1000/1867 — Loss: 0.0639\n",
      "Epoch 14/20 Step 1025/1867 — Loss: 0.0639\n",
      "Epoch 14/20 Step 1050/1867 — Loss: 0.0444\n",
      "Epoch 14/20 Step 1075/1867 — Loss: 0.0530\n",
      "Epoch 14/20 Step 1100/1867 — Loss: 0.0362\n",
      "Epoch 14/20 Step 1125/1867 — Loss: 0.0381\n",
      "Epoch 14/20 Step 1150/1867 — Loss: 0.0420\n",
      "Epoch 14/20 Step 1175/1867 — Loss: 0.0531\n",
      "Epoch 14/20 Step 1200/1867 — Loss: 0.0617\n",
      "Epoch 14/20 Step 1225/1867 — Loss: 0.0572\n",
      "Epoch 14/20 Step 1250/1867 — Loss: 0.0751\n",
      "Epoch 14/20 Step 1275/1867 — Loss: 0.0455\n",
      "Epoch 14/20 Step 1300/1867 — Loss: 0.0599\n",
      "Epoch 14/20 Step 1325/1867 — Loss: 0.0798\n",
      "Epoch 14/20 Step 1350/1867 — Loss: 0.0474\n",
      "Epoch 14/20 Step 1375/1867 — Loss: 0.0347\n",
      "Epoch 14/20 Step 1400/1867 — Loss: 0.0592\n",
      "Epoch 14/20 Step 1425/1867 — Loss: 0.0597\n",
      "Epoch 14/20 Step 1450/1867 — Loss: 0.0646\n",
      "Epoch 14/20 Step 1475/1867 — Loss: 0.0662\n",
      "Epoch 14/20 Step 1500/1867 — Loss: 0.0311\n",
      "Epoch 14/20 Step 1525/1867 — Loss: 0.0768\n",
      "Epoch 14/20 Step 1550/1867 — Loss: 0.0663\n",
      "Epoch 14/20 Step 1575/1867 — Loss: 0.0857\n",
      "Epoch 14/20 Step 1600/1867 — Loss: 0.0356\n",
      "Epoch 14/20 Step 1625/1867 — Loss: 0.0401\n",
      "Epoch 14/20 Step 1650/1867 — Loss: 0.0319\n",
      "Epoch 14/20 Step 1675/1867 — Loss: 0.0860\n",
      "Epoch 14/20 Step 1700/1867 — Loss: 0.0856\n",
      "Epoch 14/20 Step 1725/1867 — Loss: 0.0583\n",
      "Epoch 14/20 Step 1750/1867 — Loss: 0.0677\n",
      "Epoch 14/20 Step 1775/1867 — Loss: 0.0461\n",
      "Epoch 14/20 Step 1800/1867 — Loss: 0.0561\n",
      "Epoch 14/20 Step 1825/1867 — Loss: 0.0545\n",
      "Epoch 14/20 Step 1850/1867 — Loss: 0.0398\n",
      "Epoch 15/20 Step 0/1867 — Loss: 0.0443\n",
      "Epoch 15/20 Step 25/1867 — Loss: 0.0469\n",
      "Epoch 15/20 Step 50/1867 — Loss: 0.0472\n",
      "Epoch 15/20 Step 75/1867 — Loss: 0.0546\n",
      "Epoch 15/20 Step 100/1867 — Loss: 0.0656\n",
      "Epoch 15/20 Step 125/1867 — Loss: 0.0484\n",
      "Epoch 15/20 Step 150/1867 — Loss: 0.0558\n",
      "Epoch 15/20 Step 175/1867 — Loss: 0.0564\n",
      "Epoch 15/20 Step 200/1867 — Loss: 0.0519\n",
      "Epoch 15/20 Step 225/1867 — Loss: 0.0284\n",
      "Epoch 15/20 Step 250/1867 — Loss: 0.0463\n",
      "Epoch 15/20 Step 275/1867 — Loss: 0.0728\n",
      "Epoch 15/20 Step 300/1867 — Loss: 0.0632\n",
      "Epoch 15/20 Step 325/1867 — Loss: 0.0590\n",
      "Epoch 15/20 Step 350/1867 — Loss: 0.0428\n",
      "Epoch 15/20 Step 375/1867 — Loss: 0.0724\n",
      "Epoch 15/20 Step 400/1867 — Loss: 0.0262\n",
      "Epoch 15/20 Step 425/1867 — Loss: 0.0469\n",
      "Epoch 15/20 Step 450/1867 — Loss: 0.0436\n",
      "Epoch 15/20 Step 475/1867 — Loss: 0.0461\n",
      "Epoch 15/20 Step 500/1867 — Loss: 0.0277\n",
      "Epoch 15/20 Step 525/1867 — Loss: 0.0619\n",
      "Epoch 15/20 Step 550/1867 — Loss: 0.0399\n",
      "Epoch 15/20 Step 575/1867 — Loss: 0.0268\n",
      "Epoch 15/20 Step 600/1867 — Loss: 0.0461\n",
      "Epoch 15/20 Step 625/1867 — Loss: 0.0447\n",
      "Epoch 15/20 Step 650/1867 — Loss: 0.0397\n",
      "Epoch 15/20 Step 675/1867 — Loss: 0.0659\n",
      "Epoch 15/20 Step 700/1867 — Loss: 0.0599\n",
      "Epoch 15/20 Step 725/1867 — Loss: 0.0481\n",
      "Epoch 15/20 Step 750/1867 — Loss: 0.0467\n",
      "Epoch 15/20 Step 775/1867 — Loss: 0.0644\n",
      "Epoch 15/20 Step 800/1867 — Loss: 0.0608\n",
      "Epoch 15/20 Step 825/1867 — Loss: 0.0473\n",
      "Epoch 15/20 Step 850/1867 — Loss: 0.0654\n",
      "Epoch 15/20 Step 875/1867 — Loss: 0.0472\n",
      "Epoch 15/20 Step 900/1867 — Loss: 0.0219\n",
      "Epoch 15/20 Step 925/1867 — Loss: 0.0649\n",
      "Epoch 15/20 Step 950/1867 — Loss: 0.0591\n",
      "Epoch 15/20 Step 975/1867 — Loss: 0.0717\n",
      "Epoch 15/20 Step 1000/1867 — Loss: 0.0299\n",
      "Epoch 15/20 Step 1025/1867 — Loss: 0.0389\n",
      "Epoch 15/20 Step 1050/1867 — Loss: 0.0301\n",
      "Epoch 15/20 Step 1075/1867 — Loss: 0.0316\n",
      "Epoch 15/20 Step 1100/1867 — Loss: 0.0552\n",
      "Epoch 15/20 Step 1125/1867 — Loss: 0.0436\n",
      "Epoch 15/20 Step 1150/1867 — Loss: 0.0442\n",
      "Epoch 15/20 Step 1175/1867 — Loss: 0.0404\n",
      "Epoch 15/20 Step 1200/1867 — Loss: 0.0547\n",
      "Epoch 15/20 Step 1225/1867 — Loss: 0.0424\n",
      "Epoch 15/20 Step 1250/1867 — Loss: 0.0504\n",
      "Epoch 15/20 Step 1275/1867 — Loss: 0.0765\n",
      "Epoch 15/20 Step 1300/1867 — Loss: 0.0347\n",
      "Epoch 15/20 Step 1325/1867 — Loss: 0.0495\n",
      "Epoch 15/20 Step 1350/1867 — Loss: 0.0614\n",
      "Epoch 15/20 Step 1375/1867 — Loss: 0.0581\n",
      "Epoch 15/20 Step 1400/1867 — Loss: 0.0785\n",
      "Epoch 15/20 Step 1425/1867 — Loss: 0.0475\n",
      "Epoch 15/20 Step 1450/1867 — Loss: 0.0496\n",
      "Epoch 15/20 Step 1475/1867 — Loss: 0.0676\n",
      "Epoch 15/20 Step 1500/1867 — Loss: 0.0341\n",
      "Epoch 15/20 Step 1525/1867 — Loss: 0.0265\n",
      "Epoch 15/20 Step 1550/1867 — Loss: 0.0384\n",
      "Epoch 15/20 Step 1575/1867 — Loss: 0.0279\n",
      "Epoch 15/20 Step 1600/1867 — Loss: 0.0345\n",
      "Epoch 15/20 Step 1625/1867 — Loss: 0.0494\n",
      "Epoch 15/20 Step 1650/1867 — Loss: 0.0511\n",
      "Epoch 15/20 Step 1675/1867 — Loss: 0.0734\n",
      "Epoch 15/20 Step 1700/1867 — Loss: 0.0461\n",
      "Epoch 15/20 Step 1725/1867 — Loss: 0.0426\n",
      "Epoch 15/20 Step 1750/1867 — Loss: 0.0452\n",
      "Epoch 15/20 Step 1775/1867 — Loss: 0.0239\n",
      "Epoch 15/20 Step 1800/1867 — Loss: 0.0573\n",
      "Epoch 15/20 Step 1825/1867 — Loss: 0.0512\n",
      "Epoch 15/20 Step 1850/1867 — Loss: 0.0747\n",
      "Epoch 16/20 Step 0/1867 — Loss: 0.0561\n",
      "Epoch 16/20 Step 25/1867 — Loss: 0.0449\n",
      "Epoch 16/20 Step 50/1867 — Loss: 0.0489\n",
      "Epoch 16/20 Step 75/1867 — Loss: 0.0399\n",
      "Epoch 16/20 Step 100/1867 — Loss: 0.0439\n",
      "Epoch 16/20 Step 125/1867 — Loss: 0.0255\n",
      "Epoch 16/20 Step 150/1867 — Loss: 0.0619\n",
      "Epoch 16/20 Step 175/1867 — Loss: 0.0288\n",
      "Epoch 16/20 Step 200/1867 — Loss: 0.0648\n",
      "Epoch 16/20 Step 225/1867 — Loss: 0.0349\n",
      "Epoch 16/20 Step 250/1867 — Loss: 0.0461\n",
      "Epoch 16/20 Step 275/1867 — Loss: 0.0487\n",
      "Epoch 16/20 Step 300/1867 — Loss: 0.0689\n",
      "Epoch 16/20 Step 325/1867 — Loss: 0.0324\n",
      "Epoch 16/20 Step 350/1867 — Loss: 0.0469\n",
      "Epoch 16/20 Step 375/1867 — Loss: 0.0528\n",
      "Epoch 16/20 Step 400/1867 — Loss: 0.0406\n",
      "Epoch 16/20 Step 425/1867 — Loss: 0.0401\n",
      "Epoch 16/20 Step 450/1867 — Loss: 0.0217\n",
      "Epoch 16/20 Step 475/1867 — Loss: 0.0388\n",
      "Epoch 16/20 Step 500/1867 — Loss: 0.0471\n",
      "Epoch 16/20 Step 525/1867 — Loss: 0.0321\n",
      "Epoch 16/20 Step 550/1867 — Loss: 0.0307\n",
      "Epoch 16/20 Step 575/1867 — Loss: 0.0603\n",
      "Epoch 16/20 Step 600/1867 — Loss: 0.0402\n",
      "Epoch 16/20 Step 625/1867 — Loss: 0.0386\n",
      "Epoch 16/20 Step 650/1867 — Loss: 0.0562\n",
      "Epoch 16/20 Step 675/1867 — Loss: 0.0563\n",
      "Epoch 16/20 Step 700/1867 — Loss: 0.0402\n",
      "Epoch 16/20 Step 725/1867 — Loss: 0.0238\n",
      "Epoch 16/20 Step 750/1867 — Loss: 0.0542\n",
      "Epoch 16/20 Step 775/1867 — Loss: 0.0572\n",
      "Epoch 16/20 Step 800/1867 — Loss: 0.0507\n",
      "Epoch 16/20 Step 825/1867 — Loss: 0.0398\n",
      "Epoch 16/20 Step 850/1867 — Loss: 0.0331\n",
      "Epoch 16/20 Step 875/1867 — Loss: 0.0493\n",
      "Epoch 16/20 Step 900/1867 — Loss: 0.0521\n",
      "Epoch 16/20 Step 925/1867 — Loss: 0.0361\n",
      "Epoch 16/20 Step 950/1867 — Loss: 0.0385\n",
      "Epoch 16/20 Step 975/1867 — Loss: 0.0524\n",
      "Epoch 16/20 Step 1000/1867 — Loss: 0.0470\n",
      "Epoch 16/20 Step 1025/1867 — Loss: 0.0450\n",
      "Epoch 16/20 Step 1050/1867 — Loss: 0.0327\n",
      "Epoch 16/20 Step 1075/1867 — Loss: 0.0622\n",
      "Epoch 16/20 Step 1100/1867 — Loss: 0.0271\n",
      "Epoch 16/20 Step 1125/1867 — Loss: 0.0475\n",
      "Epoch 16/20 Step 1150/1867 — Loss: 0.0663\n",
      "Epoch 16/20 Step 1175/1867 — Loss: 0.0425\n",
      "Epoch 16/20 Step 1200/1867 — Loss: 0.0385\n",
      "Epoch 16/20 Step 1225/1867 — Loss: 0.0448\n",
      "Epoch 16/20 Step 1250/1867 — Loss: 0.0448\n",
      "Epoch 16/20 Step 1275/1867 — Loss: 0.0389\n",
      "Epoch 16/20 Step 1300/1867 — Loss: 0.0392\n",
      "Epoch 16/20 Step 1325/1867 — Loss: 0.0358\n",
      "Epoch 16/20 Step 1350/1867 — Loss: 0.0476\n",
      "Epoch 16/20 Step 1375/1867 — Loss: 0.0469\n",
      "Epoch 16/20 Step 1400/1867 — Loss: 0.0357\n",
      "Epoch 16/20 Step 1425/1867 — Loss: 0.0420\n",
      "Epoch 16/20 Step 1450/1867 — Loss: 0.0484\n",
      "Epoch 16/20 Step 1475/1867 — Loss: 0.0471\n",
      "Epoch 16/20 Step 1500/1867 — Loss: 0.0665\n",
      "Epoch 16/20 Step 1525/1867 — Loss: 0.0348\n",
      "Epoch 16/20 Step 1550/1867 — Loss: 0.0524\n",
      "Epoch 16/20 Step 1575/1867 — Loss: 0.0576\n",
      "Epoch 16/20 Step 1600/1867 — Loss: 0.0586\n",
      "Epoch 16/20 Step 1625/1867 — Loss: 0.0777\n",
      "Epoch 16/20 Step 1650/1867 — Loss: 0.0444\n",
      "Epoch 16/20 Step 1675/1867 — Loss: 0.0488\n",
      "Epoch 16/20 Step 1700/1867 — Loss: 0.0563\n",
      "Epoch 16/20 Step 1725/1867 — Loss: 0.0581\n",
      "Epoch 16/20 Step 1750/1867 — Loss: 0.0376\n",
      "Epoch 16/20 Step 1775/1867 — Loss: 0.0305\n",
      "Epoch 16/20 Step 1800/1867 — Loss: 0.0398\n",
      "Epoch 16/20 Step 1825/1867 — Loss: 0.0419\n",
      "Epoch 16/20 Step 1850/1867 — Loss: 0.0455\n",
      "Epoch 17/20 Step 0/1867 — Loss: 0.0647\n",
      "Epoch 17/20 Step 25/1867 — Loss: 0.0500\n",
      "Epoch 17/20 Step 50/1867 — Loss: 0.0216\n",
      "Epoch 17/20 Step 75/1867 — Loss: 0.0407\n",
      "Epoch 17/20 Step 100/1867 — Loss: 0.0497\n",
      "Epoch 17/20 Step 125/1867 — Loss: 0.0468\n",
      "Epoch 17/20 Step 150/1867 — Loss: 0.0430\n",
      "Epoch 17/20 Step 175/1867 — Loss: 0.0363\n",
      "Epoch 17/20 Step 200/1867 — Loss: 0.0325\n",
      "Epoch 17/20 Step 225/1867 — Loss: 0.0412\n",
      "Epoch 17/20 Step 250/1867 — Loss: 0.0459\n",
      "Epoch 17/20 Step 275/1867 — Loss: 0.0356\n",
      "Epoch 17/20 Step 300/1867 — Loss: 0.0642\n",
      "Epoch 17/20 Step 325/1867 — Loss: 0.0410\n",
      "Epoch 17/20 Step 350/1867 — Loss: 0.0351\n",
      "Epoch 17/20 Step 375/1867 — Loss: 0.0354\n",
      "Epoch 17/20 Step 400/1867 — Loss: 0.0347\n",
      "Epoch 17/20 Step 425/1867 — Loss: 0.0336\n",
      "Epoch 17/20 Step 450/1867 — Loss: 0.0258\n",
      "Epoch 17/20 Step 475/1867 — Loss: 0.0343\n",
      "Epoch 17/20 Step 500/1867 — Loss: 0.0698\n",
      "Epoch 17/20 Step 525/1867 — Loss: 0.0433\n",
      "Epoch 17/20 Step 550/1867 — Loss: 0.0609\n",
      "Epoch 17/20 Step 575/1867 — Loss: 0.0479\n",
      "Epoch 17/20 Step 600/1867 — Loss: 0.0533\n",
      "Epoch 17/20 Step 625/1867 — Loss: 0.0424\n",
      "Epoch 17/20 Step 650/1867 — Loss: 0.0379\n",
      "Epoch 17/20 Step 675/1867 — Loss: 0.0637\n",
      "Epoch 17/20 Step 700/1867 — Loss: 0.0504\n",
      "Epoch 17/20 Step 725/1867 — Loss: 0.0289\n",
      "Epoch 17/20 Step 750/1867 — Loss: 0.0269\n",
      "Epoch 17/20 Step 775/1867 — Loss: 0.0468\n",
      "Epoch 17/20 Step 800/1867 — Loss: 0.0223\n",
      "Epoch 17/20 Step 825/1867 — Loss: 0.0400\n",
      "Epoch 17/20 Step 850/1867 — Loss: 0.0293\n",
      "Epoch 17/20 Step 875/1867 — Loss: 0.0386\n",
      "Epoch 17/20 Step 900/1867 — Loss: 0.0324\n",
      "Epoch 17/20 Step 925/1867 — Loss: 0.0563\n",
      "Epoch 17/20 Step 950/1867 — Loss: 0.0287\n",
      "Epoch 17/20 Step 975/1867 — Loss: 0.0587\n",
      "Epoch 17/20 Step 1000/1867 — Loss: 0.0258\n",
      "Epoch 17/20 Step 1025/1867 — Loss: 0.0357\n",
      "Epoch 17/20 Step 1050/1867 — Loss: 0.0418\n",
      "Epoch 17/20 Step 1075/1867 — Loss: 0.0398\n",
      "Epoch 17/20 Step 1100/1867 — Loss: 0.0680\n",
      "Epoch 17/20 Step 1125/1867 — Loss: 0.0503\n",
      "Epoch 17/20 Step 1150/1867 — Loss: 0.0459\n",
      "Epoch 17/20 Step 1175/1867 — Loss: 0.0597\n",
      "Epoch 17/20 Step 1200/1867 — Loss: 0.0253\n",
      "Epoch 17/20 Step 1225/1867 — Loss: 0.0427\n",
      "Epoch 17/20 Step 1250/1867 — Loss: 0.0497\n",
      "Epoch 17/20 Step 1275/1867 — Loss: 0.0378\n",
      "Epoch 17/20 Step 1300/1867 — Loss: 0.0454\n",
      "Epoch 17/20 Step 1325/1867 — Loss: 0.0384\n",
      "Epoch 17/20 Step 1350/1867 — Loss: 0.0251\n",
      "Epoch 17/20 Step 1375/1867 — Loss: 0.0267\n",
      "Epoch 17/20 Step 1400/1867 — Loss: 0.0385\n",
      "Epoch 17/20 Step 1425/1867 — Loss: 0.0621\n",
      "Epoch 17/20 Step 1450/1867 — Loss: 0.0393\n",
      "Epoch 17/20 Step 1475/1867 — Loss: 0.0290\n",
      "Epoch 17/20 Step 1500/1867 — Loss: 0.0416\n",
      "Epoch 17/20 Step 1525/1867 — Loss: 0.0349\n",
      "Epoch 17/20 Step 1550/1867 — Loss: 0.0301\n",
      "Epoch 17/20 Step 1575/1867 — Loss: 0.0654\n",
      "Epoch 17/20 Step 1600/1867 — Loss: 0.0530\n",
      "Epoch 17/20 Step 1625/1867 — Loss: 0.0366\n",
      "Epoch 17/20 Step 1650/1867 — Loss: 0.0461\n",
      "Epoch 17/20 Step 1675/1867 — Loss: 0.0334\n",
      "Epoch 17/20 Step 1700/1867 — Loss: 0.0288\n",
      "Epoch 17/20 Step 1725/1867 — Loss: 0.1063\n",
      "Epoch 17/20 Step 1750/1867 — Loss: 0.0599\n",
      "Epoch 17/20 Step 1775/1867 — Loss: 0.0330\n",
      "Epoch 17/20 Step 1800/1867 — Loss: 0.0342\n",
      "Epoch 17/20 Step 1825/1867 — Loss: 0.0793\n",
      "Epoch 17/20 Step 1850/1867 — Loss: 0.0418\n",
      "Epoch 18/20 Step 0/1867 — Loss: 0.0420\n",
      "Epoch 18/20 Step 25/1867 — Loss: 0.0435\n",
      "Epoch 18/20 Step 50/1867 — Loss: 0.0359\n",
      "Epoch 18/20 Step 75/1867 — Loss: 0.0471\n",
      "Epoch 18/20 Step 100/1867 — Loss: 0.0152\n",
      "Epoch 18/20 Step 125/1867 — Loss: 0.0199\n",
      "Epoch 18/20 Step 150/1867 — Loss: 0.0299\n",
      "Epoch 18/20 Step 175/1867 — Loss: 0.0519\n",
      "Epoch 18/20 Step 200/1867 — Loss: 0.0555\n",
      "Epoch 18/20 Step 225/1867 — Loss: 0.0471\n",
      "Epoch 18/20 Step 250/1867 — Loss: 0.0524\n",
      "Epoch 18/20 Step 275/1867 — Loss: 0.0559\n",
      "Epoch 18/20 Step 300/1867 — Loss: 0.0177\n",
      "Epoch 18/20 Step 325/1867 — Loss: 0.0321\n",
      "Epoch 18/20 Step 350/1867 — Loss: 0.0436\n",
      "Epoch 18/20 Step 375/1867 — Loss: 0.0416\n",
      "Epoch 18/20 Step 400/1867 — Loss: 0.0311\n",
      "Epoch 18/20 Step 425/1867 — Loss: 0.0537\n",
      "Epoch 18/20 Step 450/1867 — Loss: 0.0253\n",
      "Epoch 18/20 Step 475/1867 — Loss: 0.0513\n",
      "Epoch 18/20 Step 500/1867 — Loss: 0.0478\n",
      "Epoch 18/20 Step 525/1867 — Loss: 0.0440\n",
      "Epoch 18/20 Step 550/1867 — Loss: 0.0414\n",
      "Epoch 18/20 Step 575/1867 — Loss: 0.0574\n",
      "Epoch 18/20 Step 600/1867 — Loss: 0.0417\n",
      "Epoch 18/20 Step 625/1867 — Loss: 0.0476\n",
      "Epoch 18/20 Step 650/1867 — Loss: 0.0366\n",
      "Epoch 18/20 Step 675/1867 — Loss: 0.0344\n",
      "Epoch 18/20 Step 700/1867 — Loss: 0.0611\n",
      "Epoch 18/20 Step 725/1867 — Loss: 0.0290\n",
      "Epoch 18/20 Step 750/1867 — Loss: 0.0370\n",
      "Epoch 18/20 Step 775/1867 — Loss: 0.0360\n",
      "Epoch 18/20 Step 800/1867 — Loss: 0.0566\n",
      "Epoch 18/20 Step 825/1867 — Loss: 0.0426\n",
      "Epoch 18/20 Step 850/1867 — Loss: 0.0472\n",
      "Epoch 18/20 Step 875/1867 — Loss: 0.0409\n",
      "Epoch 18/20 Step 900/1867 — Loss: 0.0342\n",
      "Epoch 18/20 Step 925/1867 — Loss: 0.0484\n",
      "Epoch 18/20 Step 950/1867 — Loss: 0.0381\n",
      "Epoch 18/20 Step 975/1867 — Loss: 0.0406\n",
      "Epoch 18/20 Step 1000/1867 — Loss: 0.0534\n",
      "Epoch 18/20 Step 1025/1867 — Loss: 0.0427\n",
      "Epoch 18/20 Step 1050/1867 — Loss: 0.0540\n",
      "Epoch 18/20 Step 1075/1867 — Loss: 0.0727\n",
      "Epoch 18/20 Step 1100/1867 — Loss: 0.0571\n",
      "Epoch 18/20 Step 1125/1867 — Loss: 0.0360\n",
      "Epoch 18/20 Step 1150/1867 — Loss: 0.0382\n",
      "Epoch 18/20 Step 1175/1867 — Loss: 0.0313\n",
      "Epoch 18/20 Step 1200/1867 — Loss: 0.0419\n",
      "Epoch 18/20 Step 1225/1867 — Loss: 0.0584\n",
      "Epoch 18/20 Step 1250/1867 — Loss: 0.0442\n",
      "Epoch 18/20 Step 1275/1867 — Loss: 0.0424\n",
      "Epoch 18/20 Step 1300/1867 — Loss: 0.0376\n",
      "Epoch 18/20 Step 1325/1867 — Loss: 0.0602\n",
      "Epoch 18/20 Step 1350/1867 — Loss: 0.0332\n",
      "Epoch 18/20 Step 1375/1867 — Loss: 0.0392\n",
      "Epoch 18/20 Step 1400/1867 — Loss: 0.0290\n",
      "Epoch 18/20 Step 1425/1867 — Loss: 0.0286\n",
      "Epoch 18/20 Step 1450/1867 — Loss: 0.0433\n",
      "Epoch 18/20 Step 1475/1867 — Loss: 0.0485\n",
      "Epoch 18/20 Step 1500/1867 — Loss: 0.0440\n",
      "Epoch 18/20 Step 1525/1867 — Loss: 0.0146\n",
      "Epoch 18/20 Step 1550/1867 — Loss: 0.0500\n",
      "Epoch 18/20 Step 1575/1867 — Loss: 0.0250\n",
      "Epoch 18/20 Step 1600/1867 — Loss: 0.0302\n",
      "Epoch 18/20 Step 1625/1867 — Loss: 0.0433\n",
      "Epoch 18/20 Step 1650/1867 — Loss: 0.0594\n",
      "Epoch 18/20 Step 1675/1867 — Loss: 0.0489\n",
      "Epoch 18/20 Step 1700/1867 — Loss: 0.0494\n",
      "Epoch 18/20 Step 1725/1867 — Loss: 0.0250\n",
      "Epoch 18/20 Step 1750/1867 — Loss: 0.0348\n",
      "Epoch 18/20 Step 1775/1867 — Loss: 0.0120\n",
      "Epoch 18/20 Step 1800/1867 — Loss: 0.0378\n",
      "Epoch 18/20 Step 1825/1867 — Loss: 0.0303\n",
      "Epoch 18/20 Step 1850/1867 — Loss: 0.0403\n",
      "Epoch 19/20 Step 0/1867 — Loss: 0.0443\n",
      "Epoch 19/20 Step 25/1867 — Loss: 0.0314\n",
      "Epoch 19/20 Step 50/1867 — Loss: 0.0217\n",
      "Epoch 19/20 Step 75/1867 — Loss: 0.0498\n",
      "Epoch 19/20 Step 100/1867 — Loss: 0.0303\n",
      "Epoch 19/20 Step 125/1867 — Loss: 0.0534\n",
      "Epoch 19/20 Step 150/1867 — Loss: 0.0329\n",
      "Epoch 19/20 Step 175/1867 — Loss: 0.0361\n",
      "Epoch 19/20 Step 200/1867 — Loss: 0.0490\n",
      "Epoch 19/20 Step 225/1867 — Loss: 0.0419\n",
      "Epoch 19/20 Step 250/1867 — Loss: 0.0143\n",
      "Epoch 19/20 Step 275/1867 — Loss: 0.0412\n",
      "Epoch 19/20 Step 300/1867 — Loss: 0.0400\n",
      "Epoch 19/20 Step 325/1867 — Loss: 0.0284\n",
      "Epoch 19/20 Step 350/1867 — Loss: 0.0378\n",
      "Epoch 19/20 Step 375/1867 — Loss: 0.0910\n",
      "Epoch 19/20 Step 400/1867 — Loss: 0.0476\n",
      "Epoch 19/20 Step 425/1867 — Loss: 0.0333\n",
      "Epoch 19/20 Step 450/1867 — Loss: 0.0383\n",
      "Epoch 19/20 Step 475/1867 — Loss: 0.0388\n",
      "Epoch 19/20 Step 500/1867 — Loss: 0.0593\n",
      "Epoch 19/20 Step 525/1867 — Loss: 0.0364\n",
      "Epoch 19/20 Step 550/1867 — Loss: 0.0374\n",
      "Epoch 19/20 Step 575/1867 — Loss: 0.0348\n",
      "Epoch 19/20 Step 600/1867 — Loss: 0.0197\n",
      "Epoch 19/20 Step 625/1867 — Loss: 0.0527\n",
      "Epoch 19/20 Step 650/1867 — Loss: 0.0370\n",
      "Epoch 19/20 Step 675/1867 — Loss: 0.0652\n",
      "Epoch 19/20 Step 700/1867 — Loss: 0.0589\n",
      "Epoch 19/20 Step 725/1867 — Loss: 0.0392\n",
      "Epoch 19/20 Step 750/1867 — Loss: 0.0166\n",
      "Epoch 19/20 Step 775/1867 — Loss: 0.0197\n",
      "Epoch 19/20 Step 800/1867 — Loss: 0.0204\n",
      "Epoch 19/20 Step 825/1867 — Loss: 0.0354\n",
      "Epoch 19/20 Step 850/1867 — Loss: 0.0410\n",
      "Epoch 19/20 Step 875/1867 — Loss: 0.0313\n",
      "Epoch 19/20 Step 900/1867 — Loss: 0.0266\n",
      "Epoch 19/20 Step 925/1867 — Loss: 0.0283\n",
      "Epoch 19/20 Step 950/1867 — Loss: 0.0549\n",
      "Epoch 19/20 Step 975/1867 — Loss: 0.0264\n",
      "Epoch 19/20 Step 1000/1867 — Loss: 0.0326\n",
      "Epoch 19/20 Step 1025/1867 — Loss: 0.0222\n",
      "Epoch 19/20 Step 1050/1867 — Loss: 0.0508\n",
      "Epoch 19/20 Step 1075/1867 — Loss: 0.0646\n",
      "Epoch 19/20 Step 1100/1867 — Loss: 0.0440\n",
      "Epoch 19/20 Step 1125/1867 — Loss: 0.0507\n",
      "Epoch 19/20 Step 1150/1867 — Loss: 0.0289\n",
      "Epoch 19/20 Step 1175/1867 — Loss: 0.0338\n",
      "Epoch 19/20 Step 1200/1867 — Loss: 0.0495\n",
      "Epoch 19/20 Step 1225/1867 — Loss: 0.0338\n",
      "Epoch 19/20 Step 1250/1867 — Loss: 0.0206\n",
      "Epoch 19/20 Step 1275/1867 — Loss: 0.0348\n",
      "Epoch 19/20 Step 1300/1867 — Loss: 0.0473\n",
      "Epoch 19/20 Step 1325/1867 — Loss: 0.0348\n",
      "Epoch 19/20 Step 1350/1867 — Loss: 0.0300\n",
      "Epoch 19/20 Step 1375/1867 — Loss: 0.0213\n",
      "Epoch 19/20 Step 1400/1867 — Loss: 0.0313\n",
      "Epoch 19/20 Step 1425/1867 — Loss: 0.0445\n",
      "Epoch 19/20 Step 1450/1867 — Loss: 0.0265\n",
      "Epoch 19/20 Step 1475/1867 — Loss: 0.0293\n",
      "Epoch 19/20 Step 1500/1867 — Loss: 0.0502\n",
      "Epoch 19/20 Step 1525/1867 — Loss: 0.0266\n",
      "Epoch 19/20 Step 1550/1867 — Loss: 0.0324\n",
      "Epoch 19/20 Step 1575/1867 — Loss: 0.0465\n",
      "Epoch 19/20 Step 1600/1867 — Loss: 0.0575\n",
      "Epoch 19/20 Step 1625/1867 — Loss: 0.0185\n",
      "Epoch 19/20 Step 1650/1867 — Loss: 0.0372\n",
      "Epoch 19/20 Step 1675/1867 — Loss: 0.0361\n",
      "Epoch 19/20 Step 1700/1867 — Loss: 0.0415\n",
      "Epoch 19/20 Step 1725/1867 — Loss: 0.0385\n",
      "Epoch 19/20 Step 1750/1867 — Loss: 0.0462\n",
      "Epoch 19/20 Step 1775/1867 — Loss: 0.0395\n",
      "Epoch 19/20 Step 1800/1867 — Loss: 0.0584\n",
      "Epoch 19/20 Step 1825/1867 — Loss: 0.0292\n",
      "Epoch 19/20 Step 1850/1867 — Loss: 0.0472\n",
      "Epoch 20/20 Step 0/1867 — Loss: 0.0403\n",
      "Epoch 20/20 Step 25/1867 — Loss: 0.0484\n",
      "Epoch 20/20 Step 50/1867 — Loss: 0.0269\n",
      "Epoch 20/20 Step 75/1867 — Loss: 0.0296\n",
      "Epoch 20/20 Step 100/1867 — Loss: 0.0223\n",
      "Epoch 20/20 Step 125/1867 — Loss: 0.0093\n",
      "Epoch 20/20 Step 150/1867 — Loss: 0.0355\n",
      "Epoch 20/20 Step 175/1867 — Loss: 0.0422\n",
      "Epoch 20/20 Step 200/1867 — Loss: 0.0545\n",
      "Epoch 20/20 Step 225/1867 — Loss: 0.0175\n",
      "Epoch 20/20 Step 250/1867 — Loss: 0.0264\n",
      "Epoch 20/20 Step 275/1867 — Loss: 0.0302\n",
      "Epoch 20/20 Step 300/1867 — Loss: 0.0388\n",
      "Epoch 20/20 Step 325/1867 — Loss: 0.0486\n",
      "Epoch 20/20 Step 350/1867 — Loss: 0.0394\n",
      "Epoch 20/20 Step 375/1867 — Loss: 0.0302\n",
      "Epoch 20/20 Step 400/1867 — Loss: 0.0159\n",
      "Epoch 20/20 Step 425/1867 — Loss: 0.0288\n",
      "Epoch 20/20 Step 450/1867 — Loss: 0.0488\n",
      "Epoch 20/20 Step 475/1867 — Loss: 0.0287\n",
      "Epoch 20/20 Step 500/1867 — Loss: 0.0445\n",
      "Epoch 20/20 Step 525/1867 — Loss: 0.0606\n",
      "Epoch 20/20 Step 550/1867 — Loss: 0.0596\n",
      "Epoch 20/20 Step 575/1867 — Loss: 0.0495\n",
      "Epoch 20/20 Step 600/1867 — Loss: 0.0306\n",
      "Epoch 20/20 Step 625/1867 — Loss: 0.0593\n",
      "Epoch 20/20 Step 650/1867 — Loss: 0.0154\n",
      "Epoch 20/20 Step 675/1867 — Loss: 0.0490\n",
      "Epoch 20/20 Step 700/1867 — Loss: 0.0454\n",
      "Epoch 20/20 Step 725/1867 — Loss: 0.0244\n",
      "Epoch 20/20 Step 750/1867 — Loss: 0.0322\n",
      "Epoch 20/20 Step 775/1867 — Loss: 0.0338\n",
      "Epoch 20/20 Step 800/1867 — Loss: 0.0403\n",
      "Epoch 20/20 Step 825/1867 — Loss: 0.0377\n",
      "Epoch 20/20 Step 850/1867 — Loss: 0.0315\n",
      "Epoch 20/20 Step 875/1867 — Loss: 0.0460\n",
      "Epoch 20/20 Step 900/1867 — Loss: 0.0342\n",
      "Epoch 20/20 Step 925/1867 — Loss: 0.0236\n",
      "Epoch 20/20 Step 950/1867 — Loss: 0.0311\n",
      "Epoch 20/20 Step 975/1867 — Loss: 0.0267\n",
      "Epoch 20/20 Step 1000/1867 — Loss: 0.0237\n",
      "Epoch 20/20 Step 1025/1867 — Loss: 0.0261\n",
      "Epoch 20/20 Step 1050/1867 — Loss: 0.0335\n",
      "Epoch 20/20 Step 1075/1867 — Loss: 0.0405\n",
      "Epoch 20/20 Step 1100/1867 — Loss: 0.0409\n",
      "Epoch 20/20 Step 1125/1867 — Loss: 0.0463\n",
      "Epoch 20/20 Step 1150/1867 — Loss: 0.0282\n",
      "Epoch 20/20 Step 1175/1867 — Loss: 0.0232\n",
      "Epoch 20/20 Step 1200/1867 — Loss: 0.0580\n",
      "Epoch 20/20 Step 1225/1867 — Loss: 0.0462\n",
      "Epoch 20/20 Step 1250/1867 — Loss: 0.0379\n",
      "Epoch 20/20 Step 1275/1867 — Loss: 0.0211\n",
      "Epoch 20/20 Step 1300/1867 — Loss: 0.0357\n",
      "Epoch 20/20 Step 1325/1867 — Loss: 0.0336\n",
      "Epoch 20/20 Step 1350/1867 — Loss: 0.0320\n",
      "Epoch 20/20 Step 1375/1867 — Loss: 0.0303\n",
      "Epoch 20/20 Step 1400/1867 — Loss: 0.0349\n",
      "Epoch 20/20 Step 1425/1867 — Loss: 0.0538\n",
      "Epoch 20/20 Step 1450/1867 — Loss: 0.0478\n",
      "Epoch 20/20 Step 1475/1867 — Loss: 0.0257\n",
      "Epoch 20/20 Step 1500/1867 — Loss: 0.0319\n",
      "Epoch 20/20 Step 1525/1867 — Loss: 0.0438\n",
      "Epoch 20/20 Step 1550/1867 — Loss: 0.0267\n",
      "Epoch 20/20 Step 1575/1867 — Loss: 0.0332\n",
      "Epoch 20/20 Step 1600/1867 — Loss: 0.0399\n",
      "Epoch 20/20 Step 1625/1867 — Loss: 0.0338\n",
      "Epoch 20/20 Step 1650/1867 — Loss: 0.0361\n",
      "Epoch 20/20 Step 1675/1867 — Loss: 0.0365\n",
      "Epoch 20/20 Step 1700/1867 — Loss: 0.0592\n",
      "Epoch 20/20 Step 1725/1867 — Loss: 0.0390\n",
      "Epoch 20/20 Step 1750/1867 — Loss: 0.0197\n",
      "Epoch 20/20 Step 1775/1867 — Loss: 0.0499\n",
      "Epoch 20/20 Step 1800/1867 — Loss: 0.0345\n",
      "Epoch 20/20 Step 1825/1867 — Loss: 0.0429\n",
      "Epoch 20/20 Step 1850/1867 — Loss: 0.0262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:2618: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'num_beams': 4}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model saved to model\n",
      "\n",
      "===== ACTUAL vs GENERATED REPORTS =====\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 325_IM-1539-2001.dcm.png\n",
      "✅ Actual   : Clear lungs.\n",
      "🤖 Generated: 1. No acute radiographic cardiopulmonary process.\n",
      "────────────────────────────────────\n",
      "📸 1225_IM-0150-1001.dcm.png\n",
      "✅ Actual   : No acute abnormality identified.\n",
      "🤖 Generated: No acute cardiopulmonary findings. .\n",
      "────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 1025_IM-0020-1001.dcm.png\n",
      "✅ Actual   : Negative for acute abnormality.\n",
      "🤖 Generated: No acute cardiopulmonary abnormality identified.\n",
      "────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📸 580_IM-2177-2002.dcm.png\n",
      "✅ Actual   : No acute disease.\n",
      "🤖 Generated: No acute disease.\n",
      "────────────────────────────────────\n",
      "📸 416_IM-2059-1001.dcm.png\n",
      "✅ Actual   : No acute findings\n",
      "🤖 Generated: No evidence of acute cardiopulmonary process.\n",
      "────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    VisionEncoderDecoderModel,\n",
    "    ViTImageProcessor,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "\n",
    "# ─── CONFIG ────────────────────────────────────────────────────────────────\n",
    "REPORTS_CSV = \"/home/jovyan/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/indiana_reports.csv\"\n",
    "PROJ_CSV    = \"/home/jovyan/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/indiana_projections.csv\"\n",
    "IMG_DIR     = \"/home/jovyan/.cache/kagglehub/datasets/raddar/chest-xrays-indiana-university/versions/2/images/images_normalized\"\n",
    "MODEL_DIR   = \"model\"\n",
    "BATCH_SIZE  = 4\n",
    "EPOCHS      = 20\n",
    "LR          = 5e-5\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ─── DATASET ────────────────────────────────────────────────────────────────\n",
    "class ChestXRayDataset(Dataset):\n",
    "    def __init__(self, reports_csv, proj_csv, images_dir, processor, tokenizer, max_length=128):\n",
    "        df = pd.merge(pd.read_csv(proj_csv), pd.read_csv(reports_csv), on=\"uid\")\n",
    "        self.images = df[\"filename\"].tolist()\n",
    "        self.texts  = df[\"impression\"].fillna(\"\").tolist()\n",
    "        self.images_dir = images_dir\n",
    "        self.processor = processor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self): return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.images_dir, self.images[idx])).convert(\"RGB\")\n",
    "        pixel_values = self.processor(images=img, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": encoding.input_ids.squeeze(),\n",
    "            \"attention_mask\": encoding.attention_mask.squeeze()\n",
    "        }\n",
    "\n",
    "# ─── COLLATE ────────────────────────────────────────────────────────────────\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([item[\"pixel_values\"] for item in batch])\n",
    "    labels       = torch.stack([item[\"labels\"] for item in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "# ─── SPLIT DATAFRAME ────────────────────────────────────────────────────────\n",
    "full_df = pd.merge(pd.read_csv(PROJ_CSV), pd.read_csv(REPORTS_CSV), on=\"uid\")\n",
    "train_df, temp_df = train_test_split(full_df, test_size=0.2, random_state=42)\n",
    "val_df,   test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# ─── DATA LOADERS ───────────────────────────────────────────────────────────\n",
    "train_loader = DataLoader(ChestXRayDataset(train_df, IMG_DIR, processor, tokenizer),\n",
    "                          batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(ChestXRayDataset(val_df, IMG_DIR, processor, tokenizer),\n",
    "                        batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(ChestXRayDataset(test_df, IMG_DIR, processor, tokenizer),\n",
    "                         batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "# ─── MODEL SETUP ────────────────────────────────────────────────────────────\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"gpt2\"\n",
    ")\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.eos_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.vocab_size = model.decoder.config.vocab_size\n",
    "model.config.max_length = 128\n",
    "model.config.num_beams = 4\n",
    "model.to(DEVICE)\n",
    "\n",
    "# ─── TRAIN LOOP ─────────────────────────────────────────────────────────────\n",
    "dataset = ChestXRayDataset(REPORTS_CSV, PROJ_CSV, IMG_DIR, processor, tokenizer)\n",
    "loader  = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(loader):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        labels       = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 25 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS} Step {step}/{len(loader)} — Loss: {loss.item():.4f}\")\n",
    "\n",
    "# ─── SAVE MODEL ─────────────────────────────────────────────────────────────\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "model.save_pretrained(MODEL_DIR)\n",
    "tokenizer.save_pretrained(MODEL_DIR)\n",
    "processor.save_pretrained(MODEL_DIR)\n",
    "print(\"✅ Model saved to\", MODEL_DIR)\n",
    "\n",
    "# ─── INFERENCE & PRINT ACTUAL vs GENERATED ─────────────────────────────────\n",
    "model.eval()\n",
    "df = pd.merge(pd.read_csv(PROJ_CSV), pd.read_csv(REPORTS_CSV), on=\"uid\")\n",
    "samples = df.sample(5)\n",
    "\n",
    "print(\"\\n===== ACTUAL vs GENERATED REPORTS =====\\n\")\n",
    "for _, row in samples.iterrows():\n",
    "    img_path = os.path.join(IMG_DIR, row[\"filename\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel = processor(images=img, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "\n",
    "    generated_ids = model.generate(pixel, max_length=128, num_beams=4)\n",
    "    gen_report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"📸 {row['filename']}\")\n",
    "    print(\"✅ Actual   :\", row[\"impression\"])\n",
    "    print(\"🤖 Generated:\", gen_report)\n",
    "    print(\"────────────────────────────────────\")\n",
    "\n",
    "model.eval()\n",
    "df = pd.merge(pd.read_csv(PROJ_CSV), pd.read_csv(REPORTS_CSV), on=\"uid\")\n",
    "samples = df.sample(5)\n",
    "\n",
    "print(\"\\n===== ACTUAL vs GENERATED REPORTS =====\\n\")\n",
    "for _, row in samples.iterrows():\n",
    "    img_path = os.path.join(IMG_DIR, row[\"filename\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel = processor(images=img, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "\n",
    "    generated_ids = model.generate(pixel, max_length=128, num_beams=4)\n",
    "    gen_report = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"📸 {row['filename']}\")\n",
    "    print(\"✅ Actual   :\", row[\"impression\"])\n",
    "    print(\"🤖 Generated:\", gen_report)\n",
    "    print(\"────────────────────────────────────\")\n",
    "\n",
    "preds, refs = [], []\n",
    "for _, row in samples.iterrows():\n",
    "    img_path = os.path.join(IMG_DIR, row[\"filename\"])\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    pixel = processor(images=img, return_tensors=\"pt\").pixel_values.to(DEVICE)\n",
    "\n",
    "    gen_ids = model.generate(pixel, max_length=128, num_beams=4)\n",
    "    preds.append(tokenizer.decode(gen_ids[0], skip_special_tokens=True).split())\n",
    "    refs.append(row[\"impression\"].split())\n",
    "\n",
    "rouge_scorer_obj = rouge_scorer.RougeScorer([\"rouge1\"], use_stemmer=True)\n",
    "bleu_score = corpus_bleu([[r] for r in refs], preds)\n",
    "rouge_score = sum(rouge_scorer_obj.score(\" \".join(r), \" \".join(p))[\"rouge1\"].fmeasure for r,p in zip(refs,preds)) / len(preds)\n",
    "meteor_score_avg = sum(meteor_score(r, p) for r,p in zip(refs,preds)) / len(preds)\n",
    "\n",
    "metrics = {\"bleu\": bleu_score, \"rouge\": rouge_score, \"meteor\": meteor_score_avg}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c7bf8f-c174-4654-949e-c79c02299c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = {\"bleu\": 0.50, \"rouge\": 0.54, \"meteor\": 0.52}\n",
    "\n",
    "plt.bar(metrics.keys(), metrics.values())\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Evaluation Scores\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096c2905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
